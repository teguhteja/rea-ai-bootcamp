Supervised Learning
What is Supervised Learning? Maybe that’s one of your big questions when you get into this topic. But, before digging deeper into Supervised Learning, it’s a good idea to first do a quick recap about Machine Learning and what it has to do with Supervised Learning.

Introduction to Machine Learning
Machine Learning (ML) is a subset of Artificial Intelligence (AI) that allows software applications to become more accurate in predicting outcomes without being explicitly programmed. In simple terms, it provides machines the ability to automatically learn and improve from experience.

It’s like teaching a small child how to walk. First, the kid observes how others do it, then they try to do it themselves and keep on trying until they walk perfectly. This process involves a lot of falling and getting up. Similarly, in machine learning, models are given data, with which they make predictions. Based on the outcome of those predictions—right or wrong, the models adjust their behaviours, learning and improving over time to make accurate predictions.

According to Arthur Samuel (American pioneer in AI and gaming), Machine Learning is a field of study that gives computers the ability to learn without being explicitly programmed.

Definition and types of Machine Learning Algorithms


machine-learning-algorithm
There are various machine learning algorithms:

Supervised Learning - In supervised learning, we provide the machine with labelled input data and a correct output. In other words, we guide the machine towards the right output. It’s kind of like a student learning under the guidance of a teacher. The ‘teacher’, in this case, is the label that tells the model about the data so it can learn from it. Once the model is trained with this labelled data, it can start to make predictions or decisions when new, unlabeled data is fed to it. A common example of supervised learning is classifying emails into “spam” or “not spam”.

Unsupervised Learning - Unlike supervised learning, we do not have the comfort of a ‘teacher’ or labelled data here. In unsupervised learning, we only provide input to the machine learning model but no corresponding output. That is, the model is not told the right answer. The idea is for the model to explore the data and find some structure within. A classic example of unsupervised learning is customer segmentation, where you want to group your customers into different segments based on purchasing behaviour, but you don’t have any pre-existing labels.

Recommender Systems - Imagine walking into a bookstore and having an expert assistant who knows your reading habits guide you to books that would perfectly suit your taste – wouldn’t it be a delight? This is what recommender systems strive to do in virtual environments. They are intelligent algorithms that create a personalized list of suggestions based on data about each user’s past behaviour or user-item interactions.

Reinforcement Learning - This is a bit like learning to ride a bike. You don’t know how to do it at first, but with trial and error, you learn that pedalling keeps you balanced and turning the handlebars allows you to change direction. That’s essentially how reinforcement learning works - the model learns by interacting with its environment and receiving rewards or penalties.

What is Supervised Learning?
Now, let’s dig deeper into supervised learning, as it’s the most commonly used type of machine learning.

In supervised learning, we train the model using ‘labeled’ data. That means our data set includes both the input (
) data and its corresponding correct output (
). This pair of input-output is also known as a ‘feature’ and a ‘label’. An easy way to think about this is with a recipe: the ‘feature’ is a list of ingredients (our input), and the ‘label’ is the dish that results from those ingredients (our correct output).

The algorithm analyses this input-output pair, maps the function that transforms input to output, and tries to gain understanding of such relationship so that it can apply it to unlabeled, new data.

There are two main types of problems that Supervised Learning tackles: Regression and Classification. 1. Regression: predict a number from many possible numbers (like predicting the price of a house based on its features like size, location etc.). 2. Classification: predicts only a small number of possible outputs or categories (like determining whether an email is spam or not spam, or whether a tumor is malignant or benign, or the picture is of a cat or a dog etc.)

Overall, supervised learning offers an efficient way to use known data to make meaningful predictions about new, future data. By analyzing the relationships within labeled training data, supervised learning models can then apply what they’ve learned to unlabeled, real-world data and make informed decisions or predictions.

Linear Regression
Let’s dive into one of the most basic and fundamental algorithms in the realm of Supervised Learning: Linear Regression.

import numpy as np
import matplotlib.pyplot as plt

x_train = np.array([6.50, 7.60, 12.00, 7.20, 9.75, 10.75, 9.00, 11.50, 8.60, 13.25])
y_train = np.array([77.2, 99.8, 120.5, 89.5, 113.5, 124.2, 106.0, 133.0, 101.5, 148.2])

# Plot the data points
plt.scatter(x_train, y_train, marker='x', c='b')
# Set the title
plt.title("Housing Prices")
# Set the y-axis label
plt.ylabel('Price (in 10.000s of dollars)')
# Set the x-axis label
plt.xlabel('Size (in 100s sqft)')
plt.show()



Introduction to Linear Regression
Imagine you’re a real estate agent, and you’re helping customers sell their houses at a reasonable market price. To do this, you need a way to predict a house’s market price based on its characteristics, such as the number of rooms, the year it was built, the neighborhood it’s in, etc.

This is where Linear Regression comes in!

Linear Regression is like drawing a straight line through a cloud of data points. The line represents the relationship between the independent variables (the house characteristics, or ‘features’) and the dependent variable (the house price, or ‘target variable’).

But you might be wondering, “Why is it called ‘Linear’ Regression?” Well, it’s because this approach assumes that the relationship between the independent and dependent variables is linear. This simply means that if you were to plot this relationship on a graph, you could draw a straight line, or ‘linear’ line, to represent this relationship.

In simple terms, we’re trying to draw a line of best fit through our data points that minimizes the distance between the line and all the data points. Let’s take a look below, this is how the linear regression process works:



linear-regression
Supervised Learning includes both the input features and also the output targets, The output targets are the right answers to the model we’ll learn from. To train the model, you feed the training set, both the input features and the output targets to your learning algorithm. Then your supervised learning algorithm will produce some function (
). The job with 
 is to take a new input 
 and output and estimate or a prediction (
), In machine learning, the convention is that 
 is the estimate or the prediction for 
.

The model for Linear Regression is a linear function of the input features. It’s represented as:


General Notation	Description
Training Example feature values
Training Example
Training Example targets
m	Number of training examples
parameter: weight
parameter: bias
The model for Linear Regression
If you pay attention, in a linear function there are 
 and 
. The 
 and 
 are called the parameters of the model. In machine learning parameters of the model are the variables you can adjust during training in order to improve the model. Sometimes you also hear the parameters 
 and 
 referred to as weights and bias. Now let’s take a look at what these parameters 
 and 
 do.

import numpy as np
import matplotlib.pyplot as plt

x = np.arange(0, 5)
y = np.arange(0, 5)

Case 1
If 
 = 0 and 
 = 1.5, then 
 looks like a horizontal line.

In this case the function $f (x)=0 * x + 1.5 $ so 
 is always constant. It always predicts 1.5 for the estimated 
 value.

 is always equal to 
 and here 
 is also called y-intercept because it intersects the vertical axis or y-axis on this graph.

w = 0
b = 1.5

def compute(x, w, b):
    return w * x + b

tmp_fx = compute(x, w, b,)

tmp_x = 2.5
y = w * tmp_x + b

plt.plot(x, tmp_fx, c='r')
plt.scatter(tmp_x, y, marker='o', c='b')
plt.xlim(0, 3)
plt.ylim(0, 3)
plt.title("w=0, b=1.5 -> y = 0x + 1.5")
plt.show()



Case 2
If 
 = 0.5 and 
 = 0, then $f(x)=0.5 * x $.

If 
 = 0 then the prediction is also 0, and if 
 = 2 then the prediction is 0.5 * 2 which is 1.

You get a line like this and notice that the slope is 0.5 / 1. The value of 
 gives you the slope of the line , which is 0.5

w = 0.5
b = 0

def compute(x, w, b):
    return w * x + b

tmp_fx = compute(x, w, b,)

tmp_x = 2
y = w * tmp_x + b

plt.plot(x, tmp_fx, c='r')
plt.scatter(tmp_x, y, marker='o', c='b')
plt.xlim(0, 3)
plt.ylim(0, 3)
plt.title("w=0.5, b=0 -> y = 0.5x + 0")
plt.show()



Case 3
If 
 = 0.5 and 
 = 1, then 
. and if 
 = 0, then 
, which is 1. So the line intersects the vertical axis at 
, the intersection 
 .

Also if 
 = 2, then 
, so the line looks like this.

Again, this slope is 0.5 / 1 so the value of 
 results in a slope of 0.5.

w = 0.5
b = 1

def compute(x, w, b):
    return w * x + b

tmp_fx = compute(x, w, b,)

tmp_x = 2
y = w * tmp_x + b

plt.plot(x, tmp_fx, c='r')
plt.scatter(tmp_x, y, marker='o', c='b')
plt.xlim(0, 3)
plt.ylim(0, 3)
plt.title("w=0.5, b=1 -> y = 0.5x + 1")
plt.show()



Let’s consider a simple case study by Predicting a house’s price based on its size. The train set can be represented as follows:

House Size (100 sq.ft) (
)	Price (10.000s USD) (
)
6.5	77.2
7.6	99.8
12.0	120.5
7.20	89.5
9.75	113.5
10.75	124.2
9.0	106.0
11.5	133.0
8.6	101.5
13.25	148.2
Explanation:

In this example, we have house sizes as our independent variable (input) and corresponding house price as the dependent variable (output we want to predict). Assuming a linear relationship between size and price, we can use a linear regression model to predict the price based on the size.

import numpy as np
import matplotlib.pyplot as plt

x_train = np.array([6.50, 7.60, 12.00, 7.20, 9.75, 10.75, 9.00, 11.50, 8.60, 13.25])
y_train = np.array([77.2, 99.8, 120.5, 89.5, 113.5, 124.2, 106.0, 133.0, 101.5, 148.2])

# Plot the data points
plt.scatter(x_train, y_train, marker='x', c='b')
# Set the title
plt.title("Housing Prices")
# Set the y-axis label
plt.ylabel('Price (in 10.000s of dollars)')
# Set the x-axis label
plt.xlabel('Size (in 100s sqft)')
plt.show()



Now, the question is: How to predict the price of 9.5 sqft house? We can use the Linear Regression model to predict this.

With the amount of data we have, it would be difficult if we did 1 by 1 calculations manually. So, you can compute the function output in a for loop as shown in the compute_output function below.

def compute_output(x, w, b):
    """
    Computes the prediction of a linear model
    Args:
      x    : Data, m examples 
      w,b  : model parameters  
    Returns
      f_wb : model prediction
    """
    m = len(x_train)
    f_wb = w * x + b

    return f_wb

# Please replace the values of w and b with numbers that you think are appropriate
w = 9.8
b = 15

tmp_f_wb = compute_output(x_train, w, b,)

# predict the price of 9.5 sqft house
tmp_x = 9.5
y_hat = w * tmp_x + b

# Plot the price of 9.5 sqft house
plt.scatter(tmp_x, y_hat, marker='o', c='black',label='9.5 sqft house')
plt.text(11.5, 80, f"Price = {y_hat:.2f}", fontsize=12)

# Plot our model prediction
plt.plot(x_train, tmp_f_wb, c='r',label='Our Prediction')

# Plot the data points
plt.scatter(x_train, y_train, marker='x', c='b',label='Actual Values')

# Set the title
plt.title("Housing Prices")
# Set the y-axis label
plt.ylabel('Price (in 10.000s of dollars)')
# Set the x-axis label
plt.xlabel('Size (in 100s sqft)')
plt.legend()
plt.show()



Loss function
Now, how do we know if the line is a good fit?

We can calculate the error between the predicted value and the actual value.



error-loss
For example, if the actual value is 
, and the predicted value is 
, then the error is 
.

We can calculate the error for each data point, and then sum them up.

The error function is:


where 
 is the actual value, and 
 is the predicted value.

This is called the sum of squared errors. (SSE)

import numpy as np
import matplotlib.pyplot as plt
import ipywidgets as widgets
from IPython.display import display

x_data = np.random.rand(100) * 10
noise = np.random.normal(0, 2, x_data.shape)
y_data = 3*x_data + 8 + noise

# Define the update function
def update(a, b):
    y = a*x + b
    plt.plot(x, y, color='red')
    plt.scatter(x_data, y_data, s=1)
    plt.xlabel('x')
    plt.ylabel('y')

    plt.title(f"Equation y = {a}x + {b}")

    # Calculate the loss
    loss = np.sum((y_data - y)**2)

    # Draw the loss
    plt.text(0, 20, f"Loss = {loss:.2f}", fontsize=12)

    plt.show()

# Create scattered dot around y = 3x + 8
x = np.random.rand(100) * 10
noise = np.random.normal(0, 2, x.shape)
y = 3*x + 8 + noise

# Define the slider widgets
a_slider = widgets.FloatSlider(min=0, max=10, step=0.1, value=0, description='a:')
b_slider = widgets.FloatSlider(min=0, max=10, step=0.1, value=0, description='b:')

# Display the widgets and plot
widgets.interactive(update, a=a_slider, b=b_slider)

So, the goal of linear regression is to find the best a and b that minimize the loss function.

In other words, we want to find the best a and b that make the red line as close as possible to the scattered dots.

Other Loss Functions
There are other loss functions, for example:

Mean Square Error (MSE)
 

Mean Absolute Error (MAE)

Root Mean Squared Error (RMSE)
 

RMSE is in the same units as the target variable, which can make it more interpretable in some cases.

The loss function, also called the cost function, tells us how well the model performs so we can try to make it better. The cost function itself measures the difference (error/loss rate) between the model prediction and the actual value for 
.

Let’s break down the formula for SSE to help you understand:

The cost function takes the prediction y hat and compares it to the target 
 by taking 
 minus 


Then, the formula is the sum of the absolute value of the diff

