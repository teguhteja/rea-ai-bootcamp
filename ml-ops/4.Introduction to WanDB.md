   Introduction to WanDB ‚Äì Mastering AI Bootcamp 
[Mastering AI Bootcamp](../../index.html)

Introduction to WanDB
=====================

*   Toolkit
    
    *   Python
        
        *   Jupyter Notebook
            
            *   [Jupyter Notebooks](../../01_toolkits/00_python/00_jupyter-notebooks/00_installation-and-setup.html)
                
            *   [Notebook cells](../../01_toolkits/00_python/00_jupyter-notebooks/01_notebook-cells.html)
                
            *   [Markdown and Formatting](../../01_toolkits/00_python/00_jupyter-notebooks/02_markdown-and-formatting.html)
                
            *   [Kernel Management](../../01_toolkits/00_python/00_jupyter-notebooks/03_kernel-management.html)
                
            *   [Magic Commands](../../01_toolkits/00_python/00_jupyter-notebooks/04_magic-commands.html)
                
            *   [JupyterLab](../../01_toolkits/00_python/00_jupyter-notebooks/05_jupyterlab.html)
                
        *   [Python Basics](../../01_toolkits/00_python/01_python-basics.html)
            
        *   [Control Structures](../../01_toolkits/00_python/02_control-structures.html)
            
        *   [Function](../../01_toolkits/00_python/03_function.html)
            
        *   [Data Structures](../../01_toolkits/00_python/04_data-structures.html)
            
        *   [Modules and Packages](../../01_toolkits/00_python/05_modules-and-packages.html)
            
        *   [Working with Files](../../01_toolkits/00_python/06_working-with-files.html)
            
        *   [Virtual Environments](../../01_toolkits/00_python/07_virtual-environments.html)
            
        *   OOP
            
            *   [Object oriented programming: Fitting functionality into single objects](../../01_toolkits/00_python/08_oop/00_oop.html)
                
            *   [Inheritance](../../01_toolkits/00_python/08_oop/01_oop_inheritance.html)
                
    *   Version Control
        
        *   [Introduction to Version Control](../../01_toolkits/01_version-control/00_introduction-to-version-control.html)
            
        *   [Git Introduction](../../01_toolkits/01_version-control/01_introduction-git.html)
            
        *   Git Command
            
            *   [`Init`](../../01_toolkits/01_version-control/02_git-command-local/00_init.html)
                
            *   [Snapshotting](../../01_toolkits/01_version-control/02_git-command-local/01_snapshotting.html)
                
            *   [`git remote add`](../../01_toolkits/01_version-control/02_git-command-local/02_git-remote.html)
                
            *   [Basic branch and merge](../../01_toolkits/01_version-control/02_git-command-local/03_basic-branch-and-merge.html)
                
            *   [`.gitignore`](../../01_toolkits/01_version-control/02_git-command-local/04_gitignore.html)
                
        *   [Github](../../01_toolkits/01_version-control/03_github.html)
            
    *   Fundamental of Statistics
        
        *   [Descriptive Statistics](../../01_toolkits/02_foundational_statistics/00_descriptive_statistics.html)
            
        *   [Fundamentals of Probability](../../01_toolkits/02_foundational_statistics/01_fundamentals_of_probability.html)
            
        *   [Data Analysis and Interpretation](../../01_toolkits/02_foundational_statistics/02_data_analysis_and_interpretation.html)
            
        *   [Distributions in Statistics](../../01_toolkits/02_foundational_statistics/03_distributions_statistics.html)
            
        *   [Bayesian Theory](../../01_toolkits/02_foundational_statistics/04_bayesian_theory.html)
            
    *   [Pandas](../../01_toolkits/03_pandas/00_pandas.html)
        
    *   [Visualization](../../01_toolkits/04_visualization/00_visualization.html)
        
*   * * *
    
*   Machine Learning Fundamental
    
    *   [What is Machine Learning](../../02_machine-learning-fundamental/00_what-is-machine-learning.html)
        
    *   [Introduction to Machine Learning](../../02_machine-learning-fundamental/01_machine-learning-fundamental.html)
        
    *   [Linear Regression](../../02_machine-learning-fundamental/02_linear-equation.html)
        
    *   [Supervised Learning](../../02_machine-learning-fundamental/03_supervised-learning.html)
        
    *   [Scalar](../../02_machine-learning-fundamental/04_matrix-and-vector.html)
        
    *   [Visualizing linear algebra as vector spaces](../../02_machine-learning-fundamental/05_linear-algebra.html)
        
    *   [k-Nearest Neighbors (kNN)](../../02_machine-learning-fundamental/06_knn.html)
        
    *   [Classification](../../02_machine-learning-fundamental/07_classification.html)
        
    *   [Decision Tree](../../02_machine-learning-fundamental/08_decision-tree.html)
        
    *   [Support Vector Machines](../../02_machine-learning-fundamental/09_svm.html)
        
    *   [Direction of a vector](../../02_machine-learning-fundamental/10_supplemental_svm.html)
        
    *   [Unsupervised Learning](../../02_machine-learning-fundamental/11_unsupervised-learning.html)
        
    *   [Anomaly Detection](../../02_machine-learning-fundamental/12_anomaly-detection.html)
        
    *   [Principal Component Analysis](../../02_machine-learning-fundamental/13_pca.html)
        
    *   [Covariance formula and variance](../../02_machine-learning-fundamental/14_supplemental-pca.html)
        
*   * * *
    
*   Deep Learning
    
    *   [Deep Learning](../../03_deep-learning/00_deep-learning.html)
        
    *   [Deep Learning Model](../../03_deep-learning/01_deep-learning-model.html)
        
    *   [Calculus](../../03_deep-learning/02_calculus-for-deep-learning.html)
        
    *   [Gradient Descend and Backpropagation](../../03_deep-learning/03_gradient-descent-and-backpropagation.html)
        
    *   [Pytorch Basic](../../03_deep-learning/04_pytorch-basic.html)
        
    *   [Gradient Descent with PyTorch](../../03_deep-learning/05_pytorch-gradient-descent.html)
        
    *   [Pytorch Dimension Modification](../../03_deep-learning/06_pytorch-dimension.html)
        
    *   [Pytorch Application](../../03_deep-learning/07_pytorch-application.html)
        
    *   [Pytorch MNIST](../../03_deep-learning/08_pytorch-mnist.html)
        
*   * * *
    
*   Model Usage
    
    *   Introduction
        
        *   [Hugging Face](../../04_model-usage/00-introduction/00_transformers_hugging_face.html)
            
    *   Pipeline
        
        *   [Pipelines](../../04_model-usage/01-pipeline/00_pipeline.html)
            
        *   [Choosing the Right Processing Unit for Machine Learning: CPU, GPU, or TPU?](../../04_model-usage/01-pipeline/01_cpu-gpu-tpu.html)
            
    *   Model Hub
        
        *   [Model Hub üóÉÔ∏è](../../04_model-usage/02-model-hub/00_models.html)
            
    *   Transfer Learning
        
        *   [Transfer Learning](../../04_model-usage/03-transfer-learning/00_transfer_learning.html)
            
        *   [Model Deployment](../../04_model-usage/03-transfer-learning/01_model_deployment.html)
            
    *   Gradio
        
        *   [Gradio](../../04_model-usage/04-gradio/00_gradio.html)
            
*   * * *
    
*   Database
    
    *   SQL
        
        *   [SQL Fundamentals with Python - Database](../../05_database/00_sql/00_sql-database.html)
            
        *   [SQL Fundamentals with Python - Tables](../../05_database/00_sql/01_sql-table.html)
            
        *   [SQL Fundamentals with Python - Joins](../../05_database/00_sql/02_sql-joins.html)
            
    *   Elasticsearch
        
        *   [Introduction to Elasticsearch](../../05_database/01_elasticsearch/00_introduction.html)
            
        *   [Installation and Configuration](../../05_database/01_elasticsearch/01_installation-and-configuration.html)
            
        *   [Data Modeling](../../05_database/01_elasticsearch/02_data-modeling.html)
            
        *   [Elasticsearch In Practice](../../05_database/01_elasticsearch/03_elasticsearch-in-practice.html)
            
        *   [Query](../../05_database/01_elasticsearch/04_query.html)
            
        *   [Snapshot](../../05_database/01_elasticsearch/05_snapshot.html)
            
*   * * *
    
*   Computer Vision
    
    *   CNN
        
        *   [Computer Vision](../../06_computer-vision/00_cnn/00_cnn-intro.html)
            
        *   [Hello World in Image Classification](../../06_computer-vision/00_cnn/01_nn-vs-cnn.html)
            
        *   [CIFAR10 comparison for regular Neural Network vs CNN](../../06_computer-vision/00_cnn/02_nn-vs-cnn-cifar10.html)
            
        *   [Convolution Layer](../../06_computer-vision/00_cnn/03_convolution-layer.html)
            
        *   [POOLING LAYER](../../06_computer-vision/00_cnn/04_pooling-layer.html)
            
        *   [Fully Connected Layer](../../06_computer-vision/00_cnn/05_fully-connected-layer.html)
            
        *   [Training](../../06_computer-vision/00_cnn/06_training.html)
            
        *   [Pretrained CNN Model](../../06_computer-vision/00_cnn/07_pretrained-model.html)
            
        *   [Applied CNN: Object Detection and YOLO in Action](../../06_computer-vision/00_cnn/08_object-detection.html)
            
    *   Stable Diffusion
        
        *   Introduction
            
            *   [Intro to Stable Diffusion](../../06_computer-vision/01_stable-diffusion/00_intro/intro.html)
                
        *   Basic
            
            *   [Stable Diffusion - Basic](../../06_computer-vision/01_stable-diffusion/01_basic/Stable_Diffusion_Basic.html)
                
        *   Fine Tuning
            
            *   [Stable Difussion Fine-tuning with Dreambooth](../../06_computer-vision/01_stable-diffusion/02_fine_tuning/Stable_Diffusion_Fine_tuning.html)
                
        *   ControlNet
            
            *   [ControlNet with Stable Diffusion](../../06_computer-vision/01_stable-diffusion/03_controlnet/Stable_Diffusion_ControlNet.html)
                
*   * * *
    
*   NLP
    
    *   [Intuition](../../07_nlp/00_intuition.html)
        
    *   [Preprocessing](../../07_nlp/01_preprocess.html)
        
    *   [Feature extraction](../../07_nlp/02_feature_extraction.html)
        
    *   [Second architecture: Using Word Embedding for sentiment classification](../../07_nlp/03_word_embedding_intuition.html)
        
    *   [ASCII](../../07_nlp/04_word_embedding.html)
        
    *   [Generate Word Embedding With Word2Vec](../../07_nlp/05_word2vec.html)
        
    *   [RNN](../../07_nlp/06_RNN.html)
        
    *   [Seq2Seq With RNN](../../07_nlp/07_Seq2Seq_with_RNN.html)
        
    *   [Problem With RNN](../../07_nlp/08_attention.html)
        
    *   [Understanding Different Transformer Architectures](../../07_nlp/09_understanding_different_architectures.html)
        
*   * * *
    
*   Langchain
    
    *   Prompt Engineering
        
        *   [NovelAI](../../08_langchain/00_prompt-engineering/00_NovelAI.html)
            
        *   [Intro to Prompt in Generative AI](../../08_langchain/00_prompt-engineering/01_intro-to-prompt-engineering.html)
            
    *   [API with FastAPI](../../08_langchain/01_fast_api.html)
        
    *   [LangChain: A Python Library for Building NLP Applications](../../08_langchain/02_intro_to_langchain.html)
        
    *   [Basic Usage LangChain](../../08_langchain/03-basic_langchain.html)
        
    *   Chain
        
        *   [Chain](../../08_langchain/04_chain/00_chain.html)
            
        *   [Chain Exercise](../../08_langchain/04_chain/01_chain_exercise.html)
            
        *   [Chain Exercise](../../08_langchain/04_chain/02_chain_exercise_answer.html)
            
    *   [Agent](../../08_langchain/05_agent.html)
        
    *   [Memory and LangChain](../../08_langchain/06_memory.html)
        
    *   [Memory Exercise Answer Key](../../08_langchain/06-memory-exercise-answer-key.html)
        
    *   [Communicating with Embedded Data in Documents](../../08_langchain/07.qna-with-data-in-the-document.html)
        
*   * * *
    
*   Machine Learning Operations
    
    *   Docker
        
        *   [Introduction to Docker](../../09_mlops/00_docker/00_intro_setup_docker.html)
            
        *   [Building Docker Images for Python Apps](../../09_mlops/00_docker/01_build_docker_images.html)
            
        *   [Deploying Python Apps with Docker Compose](../../09_mlops/00_docker/02_deploy_docker_images.html)
            
    *   Wan DB
        
        *   [Introduction to WanDB](../../09_mlops/01_wandb/00_wandb_intro_setup.html)
            
        *   [Dashboards in WanDB](../../09_mlops/01_wandb/01_wandb_dashboard.html)
            
        *   [Cnn and Wandb Tracking](../../09_mlops/01_wandb/cnn_and_wandb_tracking.html)
            
        *   [Fine-tuning a model on a text classification task](../../09_mlops/01_wandb/Text_Classification_on_GLUE.html)
            

On this page
------------

- [Introduction to WanDB](#introduction-to-wandb)
  - [On this page](#on-this-page)
- [Introduction to WanDB](#introduction-to-wandb-1)
  - [A. What is WanDB?](#a-what-is-wandb)
  - [B. Why use WanDB?](#b-why-use-wandb)
  - [C. Understanding the Importance of Monitoring and Improving Model Training](#c-understanding-the-importance-of-monitoring-and-improving-model-training)
- [Setting up WanDB](#setting-up-wandb)
  - [A. Account Setup](#a-account-setup)
  - [B. Installing the Wandb library](#b-installing-the-wandb-library)
  - [C. Initializing WanDB in Your Code](#c-initializing-wandb-in-your-code)
- [Project: CIFAR-10 Image Classification with Pytorch and WandB](#project-cifar-10-image-classification-with-pytorch-and-wandb)
  - [Overview](#overview)
  - [Steps](#steps)
    - [1. Setting Up the Environment](#1-setting-up-the-environment)
    - [2. Import the Libraries](#2-import-the-libraries)
    - [3. Initialize WandB](#3-initialize-wandb)
    - [4. Prepare the Data](#4-prepare-the-data)
    - [5. Define the Model](#5-define-the-model)
    - [6. Set Up the Loss Function and Optimizer](#6-set-up-the-loss-function-and-optimizer)
    - [7. Train the Model](#7-train-the-model)
    - [8. Evaluate the Model](#8-evaluate-the-model)
    - [Run history:](#run-history)
    - [Run summary:](#run-summary)
  - [Basic Concepts of WanDB](#basic-concepts-of-wandb)
    - [A. Projects](#a-projects)
    - [B. Runs](#b-runs)
    - [C. Artifacts](#c-artifacts)
    - [D. Sweep](#d-sweep)
    - [1. Setting Up the Configuration](#1-setting-up-the-configuration)
    - [2. Initialize the Sweep](#2-initialize-the-sweep)
    - [3. Define the Train Function](#3-define-the-train-function)
    - [4. Run the Sweep](#4-run-the-sweep)
    - [Run history:](#run-history-1)
    - [Run summary:](#run-summary-1)
    - [Run history:](#run-history-2)
    - [Run summary:](#run-summary-2)
    - [Run history:](#run-history-3)
    - [Run summary:](#run-summary-3)
    - [Run history:](#run-history-4)
    - [Run summary:](#run-summary-4)
    - [Run history:](#run-history-5)
    - [Run summary:](#run-summary-5)

Introduction to WanDB
=====================

A. What is WanDB?
-----------------

Weights & Biases (WanDB) is a developer-oriented toolset, designed specifically for machine learning. It helps monitor and visualize the model‚Äôs training process and its performance in a more intuitive way. WanDB provides a centralized platform where teams can log, share, and collaborate on their machine learning projects, making it easier to compare different runs and models‚Äô performance.

B. Why use WanDB?
-----------------

There are several reasons why WanDB stands out as a preferred tool for machine learning projects:

1.  **Track and Visualize Models**: WanDB provides a simple way to track every detail of your experiment, providing real-time visualization of your models‚Äô training and results.
    
2.  **Hyperparameter Optimization**: With WanDB‚Äôs Sweeps, you can automate hyperparameter tuning and explore the parameter space more efficiently to optimize your model‚Äôs performance.
    
3.  **Collaboration**: WanDB makes it easy to share your experiment results with colleagues and the community, fostering collaboration and knowledge sharing.
    
4.  **Reproducibility**: By logging all the metadata from your runs, WanDB helps maintain the reproducibility of your experiments, which is crucial in machine learning projects.
    

C. Understanding the Importance of Monitoring and Improving Model Training
--------------------------------------------------------------------------

Monitoring and improving model training is an essential part of the machine learning workflow.

*   **Monitoring**: By keeping track of various metrics such as loss and accuracy during the training process, you can understand how well your model is learning and whether it‚Äôs improving over time. This can help you detect issues like overfitting or underfitting early on and take corrective actions.
    
*   **Improving**: Once you monitor your model‚Äôs performance, the next step is to improve it. This could involve tweaking the model‚Äôs architecture, optimizing hyperparameters, or using more/ different data for training. Tools like WanDB make it easier to experiment with these aspects and track the impact of each change, thereby helping you build better models.
    

Setting up WanDB
================

A. Account Setup
----------------

To get started with WanDB, you need to create a free account:

1.  Visit the official Weights & Biases website: [wandb.ai](https://wandb.ai/site)
2.  Click on `Sign Up` at the top right corner of the home page.
3.  You can opt to sign up using a GitHub, Google, or LinkedIn account. Alternatively, sign up using your email address and a password.
4.  You will see your API Keys/Token, you will need it. If you didn‚Äôt save it or you lost it, you can look it up in `User Settings`.

B. Installing the Wandb library
-------------------------------

Once you‚Äôve set up your account, you need to install the Wandb library in your Python environment. It can be installed using pip:

    pip install wandb

Or with conda:

    conda install -c conda-forge wandb

Ensure you have the latest version of the library for optimal functionality.

C. Initializing WanDB in Your Code
----------------------------------

After installing the Wandb library, you need to import it and initialize it within your project. Start by importing wandb:

    import wandb

Then, initialize wandb with `wandb.init()`. You can pass several optional parameters to `wandb.init()`, such as:

*   `project`: The name of the project where you‚Äôre logging runs. This could be any string, and a new project will be created if it doesn‚Äôt already exist.
*   `entity`: The username or team name under which the project is to be created.

An example of initializing Wandb for a project named ‚Äòmy\_project‚Äô under username ‚Äòmy\_username‚Äô would be:

    wandb.init(project='my_project', entity='my_username')

After running `wandb.init()`, a new run will be created on the WanDB website, where you can track your model‚Äôs progress, visualize results, and more.

Let‚Äôs create a simple project that utilizes WandB (Weights and Biases) for experiment tracking. This project will be about classifying the CIFAR-10 dataset using a Convolutional Neural Network (CNN) implemented in Pytorch.

Project: CIFAR-10 Image Classification with Pytorch and WandB
=============================================================

Overview
--------

The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. The goal of this project is to classify the images into these classes.

We will use a Convolutional Neural Network (CNN) in Pytorch to perform this classification task. The model‚Äôs performance will be logged and visualized using WandB, a tool for machine learning experiment tracking.

Steps
-----

### 1\. Setting Up the Environment

Install the necessary libraries.

    !pip install torch torchvision wandb

    Requirement already satisfied: torch in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (2.0.1)
    Requirement already satisfied: torchvision in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (0.15.2)
    Requirement already satisfied: wandb in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (0.16.4)
    Requirement already satisfied: filelock in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from torch) (3.12.3)
    Requirement already satisfied: typing-extensions in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from torch) (4.7.1)
    Requirement already satisfied: sympy in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from torch) (1.12)
    Requirement already satisfied: networkx in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from torch) (3.1)
    Requirement already satisfied: jinja2 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from torch) (3.1.2)
    Requirement already satisfied: numpy in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from torchvision) (1.25.2)
    Requirement already satisfied: requests in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from torchvision) (2.31.0)
    Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from torchvision) (10.0.0)
    Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (8.1.7)
    Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (3.1.36)
    Requirement already satisfied: psutil>=5.0.0 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (5.9.5)
    Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (1.40.6)
    Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (0.4.0)
    Requirement already satisfied: PyYAML in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (6.0.1)
    Requirement already satisfied: setproctitle in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (1.3.3)
    Requirement already satisfied: setuptools in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (68.2.2)
    Requirement already satisfied: appdirs>=1.4.3 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (1.4.4)
    Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (4.24.3)
    Requirement already satisfied: six>=1.4.0 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)
    Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)
    Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from requests->torchvision) (3.2.0)
    Requirement already satisfied: idna<4,>=2.5 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from requests->torchvision) (3.4)
    Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)
    Requirement already satisfied: certifi>=2017.4.17 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from requests->torchvision) (2024.2.2)
    Requirement already satisfied: MarkupSafe>=2.0 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)
    Requirement already satisfied: mpmath>=0.19 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from sympy->torch) (1.3.0)
    Requirement already satisfied: smmap<6,>=3.0.1 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)
    
    [notice] A new release of pip is available: 23.3.2 -> 24.0
    [notice] To update, run: pip install --upgrade pip

### 2\. Import the Libraries

    import torch
    from torch import nn, optim
    import torchvision
    from torchvision import datasets, transforms
    import wandb
    import torch.nn.functional as F

### 3\. Initialize WandB

    wandb.login()
    run = wandb.init(project='cifar10-classification', entity='ricky-kurniawan')

    wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin

{"model\_id":"bdd352a485644c2e87a6e66ac774580c","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

Tracking run with wandb version 0.16.4

Run data is saved locally in `/Users/ruangguru/Projects/ai-bootcamp/09_mlops/01_wandb/wandb/run-20240307_162118-scdpzfhm`

Syncing run **[pretty-dew-44](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/scdpzfhm)** to [Weights & Biases](https://wandb.ai/ricky-kurniawan/cifar10-classification) ([docs](https://wandb.me/run))  

View project at [https://wandb.ai/ricky-kurniawan/cifar10-classification](https://wandb.ai/ricky-kurniawan/cifar10-classification)

View run at [https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/scdpzfhm](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/scdpzfhm)

### 4\. Prepare the Data

Load the CIFAR-10 dataset. Normalize the data and create dataloaders.

    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
    
    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)
    
    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)

    Files already downloaded and verified
    Files already downloaded and verified

### 5\. Define the Model

Define a simple CNN model.

    class Net(nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.conv1 = nn.Conv2d(3, 6, 5)
            self.pool = nn.MaxPool2d(2, 2)
            self.conv2 = nn.Conv2d(6, 16, 5)
            self.fc1 = nn.Linear(16 * 5 * 5, 120)
            self.fc2 = nn.Linear(120, 84)
            self.fc3 = nn.Linear(84, 10)
    
        def forward(self, x):
            x = self.pool(F.relu(self.conv1(x)))
            x = self.pool(F.relu(self.conv2(x)))
            x = x.view(-1, 16 * 5 * 5)
            x = F.relu(self.fc1(x))
            x = F.relu(self.fc2(x))
            x = self.fc3(x)
            return x
    
    model = Net()

### 6\. Set Up the Loss Function and Optimizer

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

### 7\. Train the Model

Train the model and log the loss and accuracy to WandB.

    def train_model(run, model, criterion, optimizer, trainloader):
        for epoch in range(10):  
            running_loss = 0.0
            correct_predictions = 0
            total_predictions = 0
            
            for i, data in enumerate(trainloader, 0):
                inputs, labels = data
    
                optimizer.zero_grad()
    
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
    
                running_loss += loss.item()
    
                _, predicted = torch.max(outputs.data, 1)
                total_predictions += labels.size(0)
                correct_predictions += (predicted == labels).sum().item()
    
                if i % 200 == 199:    # Every 200 mini-batches
                    print('[Epoch %d, Mini-batch %5d] Loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))
                    wandb.log({'Epoch': epoch + 1, 'Loss': running_loss / 2000})
                    wandb.log({'Epoch': epoch + 1, 'Loss': running_loss / 2000, 'Accuracy': correct_predictions / total_predictions * 100})
                    running_loss = 0.0
                    correct_predictions = 0
                    total_predictions = 0
    
    train_model(run, model, criterion, optimizer, trainloader)

    [Epoch 1, Mini-batch   200] Loss: 0.230
    [Epoch 1, Mini-batch   400] Loss: 0.230
    [Epoch 1, Mini-batch   600] Loss: 0.229
    [Epoch 2, Mini-batch   200] Loss: 0.218
    [Epoch 2, Mini-batch   400] Loss: 0.209
    [Epoch 2, Mini-batch   600] Loss: 0.200
    [Epoch 3, Mini-batch   200] Loss: 0.193
    [Epoch 3, Mini-batch   400] Loss: 0.189
    [Epoch 3, Mini-batch   600] Loss: 0.183
    [Epoch 4, Mini-batch   200] Loss: 0.179
    [Epoch 4, Mini-batch   400] Loss: 0.175
    [Epoch 4, Mini-batch   600] Loss: 0.171
    [Epoch 5, Mini-batch   200] Loss: 0.166
    [Epoch 5, Mini-batch   400] Loss: 0.163
    [Epoch 5, Mini-batch   600] Loss: 0.161
    [Epoch 6, Mini-batch   200] Loss: 0.155
    [Epoch 6, Mini-batch   400] Loss: 0.153
    [Epoch 6, Mini-batch   600] Loss: 0.151
    [Epoch 7, Mini-batch   200] Loss: 0.148
    [Epoch 7, Mini-batch   400] Loss: 0.145
    [Epoch 7, Mini-batch   600] Loss: 0.145
    [Epoch 8, Mini-batch   200] Loss: 0.141
    [Epoch 8, Mini-batch   400] Loss: 0.139
    [Epoch 8, Mini-batch   600] Loss: 0.142
    [Epoch 9, Mini-batch   200] Loss: 0.137
    [Epoch 9, Mini-batch   400] Loss: 0.136
    [Epoch 9, Mini-batch   600] Loss: 0.135
    [Epoch 10, Mini-batch   200] Loss: 0.133
    [Epoch 10, Mini-batch   400] Loss: 0.132
    [Epoch 10, Mini-batch   600] Loss: 0.129

### 8\. Evaluate the Model

Evaluate the model on the test data and log the test accuracy to WandB.

    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))
    
    wandb.log({'Test Accuracy': 100.0*correct/total})

    Accuracy of the network on the test images: 52 %

This section of the code calculates the accuracy of the model on the test set and logs this test accuracy to WandB for visualization and tracking.

At this point, you can go to the WandB website, navigate to your project, and see a live visualization of your model‚Äôs loss and accuracy throughout the training process. This helps to understand how well the model is learning and provides insights for further improvements.

Finally, don‚Äôt forget to close your WandB run after you‚Äôre done:

    run.finish()

{"model\_id":"4a2f6109acda4e33ad0a268819f7d67d","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%} .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% } .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }

### Run history:

  

Accuracy

‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

Epoch

‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà

Loss

‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ

Test Accuracy

‚ñÅ

  

### Run summary:

  

Accuracy

53.6875

Epoch

10

Loss

0.12919

Test Accuracy

52.62

  

View run **pretty-dew-44** at: [https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/scdpzfhm](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/scdpzfhm)  
View job at [https://wandb.ai/ricky-kurniawan/cifar10-classification/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NjA0NTAwNg==/version\_details/v3](https://wandb.ai/ricky-kurniawan/cifar10-classification/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NjA0NTAwNg==/version_details/v3)  
Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)

Find logs at: `./wandb/run-20240307_162118-scdpzfhm/logs`

This ensures all resources are properly freed and all logs are uploaded to the WandB server. This step is crucial to make sure all your model training progress and metrics are properly saved and can be reviewed later in the WandB dashboard.

Basic Concepts of WanDB
-----------------------

Here‚Äôs what the WanDB dashboard looks like:

//<!\[CDATA\[ window.\_\_mirage2 = {petok:"jgm3M6Tijz9n9uoHvxu8DiorQmDhWwo5QMAbRdxepIg-1800-0.0.1.1"}; //\]\]> 

![](https://storage.googleapis.com/rg-ai-bootcamp/mlops/wandb-project.png)

### A. Projects

In Wandb, a **project** is a collection of related machine learning experiments (known as ‚Äúruns‚Äù). It provides a shared space where you and your team can compare results, share insights, and discuss potential improvements. Each project has a dedicated page on the Wandb web application, showcasing visualizations, comparisons, and other useful metrics. We set the project name when we do `wandb.init()` in Step-3 above.

### B. Runs

A **run** in Wandb is a single execution of your machine learning script. During a run, you can log various metrics, such as loss and accuracy, system performance data, and even media like images or 3D objects. Each run gets its page in the Wandb web application, where you can view and analyze logged data.

As you can see from the screenshot of the WanDB dashboard, on the left we have 5 runs in our project.

### C. Artifacts

**Artifacts** in Wandb are used to handle version control of datasets, models, and other result files from runs. They help to track the inputs and outputs of your runs, providing a clear and useful lineage of your models and data. For example, an input artifact could be your training dataset, while output artifacts could be your trained model or predictions.

On the left you can click on the Artifacts navigation icon which will take you to:

![](https://storage.googleapis.com/rg-ai-bootcamp/mlops/wandb-artifacts.png)

In this case, the artifact is a Jupyter notebook file (`job-git_github.com_ruang-guru_ai-bootcamp.git_09_mlops_01_wandb_00_wandb_intro_setup.ipynb`) from the project `cifar10-classification` owned by the user `ricky-kurniawan`. The version of the artifact is specified after the colon - in this case, `v1` indicates it‚Äôs the first version.

The `artifact.download()` command is used to download the artifact to the local machine for use in the current run.

In the ‚ÄúUsed By‚Äù section, the listed items are the runs that have used this artifact. For example, the run `proud-salad-5` used this artifact. Information such as the run‚Äôs performance metrics, the project, the user, the artifact used, and the time of artifact creation is displayed.

In this case, `run-yqpg9z5p-history:v0` is an output artifact of the run. This run history artifact contains information about the run, such as the logged metrics. This artifact is created automatically by W&B when you log metrics or other information during a run. This allows you to revisit the specifics of a run, analyze the performance, and potentially identify areas for improvement or further exploration.

### D. Sweep

**Sweep** is a feature in Wandb for hyperparameter optimization. A sweep involves a set of runs, each with different hyperparameters, allowing you to explore a range of possibilities and identify the best parameters for your model. Wandb automates this process, generating a set of permutations of hyperparameters (based on a configuration file you create), running them, and logging the results. This makes it easier to optimize your model‚Äôs performance.

Let‚Äôs try doing a Sweep using our CIFAR-10 Project.

### 1\. Setting Up the Configuration

First, we need to create a configuration for our sweep. This configuration will specify the range and distribution of hyperparameters for the sweep. Here‚Äôs a basic example:

    sweep_config = {
        'method': 'random', #grid, random
        'metric': {
          'name': 'accuracy',
          'goal': 'maximize'   
        },
        'parameters': {
            'epochs': {
                'values': [5, 10, 15]
            },
            'batch_size': {
                'values': [16, 32, 64]
            },
            'learning_rate': {
                'min': 1e-5,
                'max': 0.1
            },
        }
    }

In this configuration, we‚Äôre specifying that we want to use a random search method (other options are `grid` for grid search and `bayes` for Bayesian optimization), and that our goal is to maximize accuracy. We‚Äôre also specifying the range of values for the hyperparameters that we want to optimize: epochs, batch size, and learning rate.

### 2\. Initialize the Sweep

    sweep_id = wandb.sweep(sweep_config, project="cifar10-classification")

    Create sweep with ID: kqyufn28
    Sweep URL: https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28

This command initializes the sweep and returns a sweep ID. This ID uniquely identifies the sweep in WandB.

### 3\. Define the Train Function

Next, we need to modify the training function to accept configurations and log them to WandB. Add the following lines at the beginning of the function:

    # Define a global counter
    global_counter = 0
    max_runs = 5
    
    def train():
        global global_counter
        if global_counter >= max_runs:
            return
        global_counter += 1
        with wandb.init(config=sweep_config):
            config = wandb.config
            model = Net() 
            criterion = nn.CrossEntropyLoss()
            optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9)
            train_model(run, model, criterion, optimizer, trainloader)

In this function, `wandb.init()` is called with the sweep configuration to start a new run. `wandb.config` is then used to access the hyperparameters for the current run.

For the purpose of teaching, we are limiting the run to a maksimum of 5. Naturally you should let the sweep run and try out all possible combinations which may take a long time. You can safely remove all lines containing ‚Äúglobal\_counter‚Äù for real case study.

### 4\. Run the Sweep

    wandb.agent(sweep_id, train)

    wandb: Agent Starting Run: jbxkaalg with config:
    wandb:  batch_size: 64
    wandb:  epochs: 10
    wandb:  learning_rate: 0.052303023093622406
    wandb: Currently logged in as: ricky-kurniawan. Use `wandb login --relogin` to force relogin

Tracking run with wandb version 0.16.4

Run data is saved locally in `/Users/ruangguru/Projects/ai-bootcamp/09_mlops/01_wandb/wandb/run-20240307_162427-jbxkaalg`

Syncing run **[gentle-sweep-1](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/jbxkaalg)** to [Weights & Biases](https://wandb.ai/ricky-kurniawan/cifar10-classification) ([docs](https://wandb.me/run))  
Sweep page: [https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28](https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28)

View project at [https://wandb.ai/ricky-kurniawan/cifar10-classification](https://wandb.ai/ricky-kurniawan/cifar10-classification)

View sweep at [https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28](https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28)

View run at [https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/jbxkaalg](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/jbxkaalg)

    [Epoch 1, Mini-batch   200] Loss: 0.205
    [Epoch 1, Mini-batch   400] Loss: 0.171
    [Epoch 1, Mini-batch   600] Loss: 0.161
    [Epoch 2, Mini-batch   200] Loss: 0.152
    [Epoch 2, Mini-batch   400] Loss: 0.148
    [Epoch 2, Mini-batch   600] Loss: 0.145
    [Epoch 3, Mini-batch   200] Loss: 0.141
    [Epoch 3, Mini-batch   400] Loss: 0.141
    [Epoch 3, Mini-batch   600] Loss: 0.140
    [Epoch 4, Mini-batch   200] Loss: 0.137
    [Epoch 4, Mini-batch   400] Loss: 0.136
    [Epoch 4, Mini-batch   600] Loss: 0.137
    [Epoch 5, Mini-batch   200] Loss: 0.131
    [Epoch 5, Mini-batch   400] Loss: 0.134
    [Epoch 5, Mini-batch   600] Loss: 0.132
    [Epoch 6, Mini-batch   200] Loss: 0.130
    [Epoch 6, Mini-batch   400] Loss: 0.132
    [Epoch 6, Mini-batch   600] Loss: 0.136
    [Epoch 7, Mini-batch   200] Loss: 0.127
    [Epoch 7, Mini-batch   400] Loss: 0.134
    [Epoch 7, Mini-batch   600] Loss: 0.135
    [Epoch 8, Mini-batch   200] Loss: 0.129
    [Epoch 8, Mini-batch   400] Loss: 0.132
    [Epoch 8, Mini-batch   600] Loss: 0.129
    [Epoch 9, Mini-batch   200] Loss: 0.125
    [Epoch 9, Mini-batch   400] Loss: 0.130
    [Epoch 9, Mini-batch   600] Loss: 0.132
    [Epoch 10, Mini-batch   200] Loss: 0.127
    [Epoch 10, Mini-batch   400] Loss: 0.128
    [Epoch 10, Mini-batch   600] Loss: 0.135

{"model\_id":"d408f9af0be44025ac3114716527b550","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%} .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% } .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }

### Run history:

  

Accuracy

‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá

Epoch

‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà

Loss

‚ñà‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ

  

### Run summary:

  

Accuracy

54.84375

Epoch

10

Loss

0.13498

  

View run **gentle-sweep-1** at: [https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/jbxkaalg](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/jbxkaalg)  
Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)

Find logs at: `./wandb/run-20240307_162427-jbxkaalg/logs`

    wandb: Agent Starting Run: hil12twe with config:
    wandb:  batch_size: 64
    wandb:  epochs: 15
    wandb:  learning_rate: 0.0778152359691495

Tracking run with wandb version 0.16.4

Run data is saved locally in `/Users/ruangguru/Projects/ai-bootcamp/09_mlops/01_wandb/wandb/run-20240307_162717-hil12twe`

Syncing run **[balmy-sweep-2](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/hil12twe)** to [Weights & Biases](https://wandb.ai/ricky-kurniawan/cifar10-classification) ([docs](https://wandb.me/run))  
Sweep page: [https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28](https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28)

View project at [https://wandb.ai/ricky-kurniawan/cifar10-classification](https://wandb.ai/ricky-kurniawan/cifar10-classification)

View sweep at [https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28](https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28)

View run at [https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/hil12twe](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/hil12twe)

    [Epoch 1, Mini-batch   200] Loss: 0.206
    [Epoch 1, Mini-batch   400] Loss: 0.184
    [Epoch 1, Mini-batch   600] Loss: 0.173
    [Epoch 2, Mini-batch   200] Loss: 0.162
    [Epoch 2, Mini-batch   400] Loss: 0.164
    [Epoch 2, Mini-batch   600] Loss: 0.164
    [Epoch 3, Mini-batch   200] Loss: 0.168
    [Epoch 3, Mini-batch   400] Loss: 0.161
    [Epoch 3, Mini-batch   600] Loss: 0.163
    [Epoch 4, Mini-batch   200] Loss: 0.160
    [Epoch 4, Mini-batch   400] Loss: 0.161
    [Epoch 4, Mini-batch   600] Loss: 0.161
    [Epoch 5, Mini-batch   200] Loss: 0.159
    [Epoch 5, Mini-batch   400] Loss: 0.162
    [Epoch 5, Mini-batch   600] Loss: 0.166
    [Epoch 6, Mini-batch   200] Loss: 0.164
    [Epoch 6, Mini-batch   400] Loss: 0.166
    [Epoch 6, Mini-batch   600] Loss: 0.169
    [Epoch 7, Mini-batch   200] Loss: 0.164
    [Epoch 7, Mini-batch   400] Loss: 0.166
    [Epoch 7, Mini-batch   600] Loss: 0.167
    [Epoch 8, Mini-batch   200] Loss: 0.171
    [Epoch 8, Mini-batch   400] Loss: 0.169
    [Epoch 8, Mini-batch   600] Loss: 0.169
    [Epoch 9, Mini-batch   200] Loss: 0.166
    [Epoch 9, Mini-batch   400] Loss: 0.173
    [Epoch 9, Mini-batch   600] Loss: 0.174
    [Epoch 10, Mini-batch   200] Loss: 0.169
    [Epoch 10, Mini-batch   400] Loss: 0.172
    [Epoch 10, Mini-batch   600] Loss: 0.173

{"model\_id":"068876cfd3b04832bcb16508589d2989","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%} .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% } .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }

### Run history:

  

Accuracy

‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá

Epoch

‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà

Loss

‚ñà‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ

  

### Run summary:

  

Accuracy

40.67188

Epoch

10

Loss

0.17274

  

View run **balmy-sweep-2** at: [https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/hil12twe](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/hil12twe)  
Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)

Find logs at: `./wandb/run-20240307_162717-hil12twe/logs`

    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: wtl4q1ha with config:
    wandb:  batch_size: 16
    wandb:  epochs: 15
    wandb:  learning_rate: 0.010806903248724136

Tracking run with wandb version 0.16.4

Run data is saved locally in `/Users/ruangguru/Projects/ai-bootcamp/09_mlops/01_wandb/wandb/run-20240307_163023-wtl4q1ha`

Syncing run **[worldly-sweep-3](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/wtl4q1ha)** to [Weights & Biases](https://wandb.ai/ricky-kurniawan/cifar10-classification) ([docs](https://wandb.me/run))  
Sweep page: [https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28](https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28)

View project at [https://wandb.ai/ricky-kurniawan/cifar10-classification](https://wandb.ai/ricky-kurniawan/cifar10-classification)

View sweep at [https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28](https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28)

View run at [https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/wtl4q1ha](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/wtl4q1ha)

    [Epoch 1, Mini-batch   200] Loss: 0.224
    [Epoch 1, Mini-batch   400] Loss: 0.190
    [Epoch 1, Mini-batch   600] Loss: 0.167
    [Epoch 2, Mini-batch   200] Loss: 0.146
    [Epoch 2, Mini-batch   400] Loss: 0.141
    [Epoch 2, Mini-batch   600] Loss: 0.136
    [Epoch 3, Mini-batch   200] Loss: 0.126
    [Epoch 3, Mini-batch   400] Loss: 0.124
    [Epoch 3, Mini-batch   600] Loss: 0.120
    [Epoch 4, Mini-batch   200] Loss: 0.114
    [Epoch 4, Mini-batch   400] Loss: 0.111
    [Epoch 4, Mini-batch   600] Loss: 0.113
    [Epoch 5, Mini-batch   200] Loss: 0.105
    [Epoch 5, Mini-batch   400] Loss: 0.104
    [Epoch 5, Mini-batch   600] Loss: 0.104
    [Epoch 6, Mini-batch   200] Loss: 0.099
    [Epoch 6, Mini-batch   400] Loss: 0.099
    [Epoch 6, Mini-batch   600] Loss: 0.099
    [Epoch 7, Mini-batch   200] Loss: 0.091
    [Epoch 7, Mini-batch   400] Loss: 0.092
    [Epoch 7, Mini-batch   600] Loss: 0.095
    [Epoch 8, Mini-batch   200] Loss: 0.087
    [Epoch 8, Mini-batch   400] Loss: 0.088
    [Epoch 8, Mini-batch   600] Loss: 0.091
    [Epoch 9, Mini-batch   200] Loss: 0.081
    [Epoch 9, Mini-batch   400] Loss: 0.085
    [Epoch 9, Mini-batch   600] Loss: 0.086
    [Epoch 10, Mini-batch   200] Loss: 0.079
    [Epoch 10, Mini-batch   400] Loss: 0.082
    [Epoch 10, Mini-batch   600] Loss: 0.083

{"model\_id":"b6f407384616488cb04ac7cb36d2550b","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%} .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% } .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }

### Run history:

  

Accuracy

‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

Epoch

‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà

Loss

‚ñà‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ

  

### Run summary:

  

Accuracy

70.96094

Epoch

10

Loss

0.083

  

View run **worldly-sweep-3** at: [https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/wtl4q1ha](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/wtl4q1ha)  
Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)

Find logs at: `./wandb/run-20240307_163023-wtl4q1ha/logs`

    wandb: Agent Starting Run: ettfrm5h with config:
    wandb:  batch_size: 64
    wandb:  epochs: 5
    wandb:  learning_rate: 0.0795927437531589

{"model\_id":"0c14053ca5e544ca8671a15db9d4d3e7","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

Tracking run with wandb version 0.16.4

Run data is saved locally in `/Users/ruangguru/Projects/ai-bootcamp/09_mlops/01_wandb/wandb/run-20240307_163323-ettfrm5h`

Syncing run **[sparkling-sweep-4](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/ettfrm5h)** to [Weights & Biases](https://wandb.ai/ricky-kurniawan/cifar10-classification) ([docs](https://wandb.me/run))  
Sweep page: [https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28](https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28)

View project at [https://wandb.ai/ricky-kurniawan/cifar10-classification](https://wandb.ai/ricky-kurniawan/cifar10-classification)

View sweep at [https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28](https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28)

View run at [https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/ettfrm5h](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/ettfrm5h)

    [Epoch 1, Mini-batch   200] Loss: 0.201
    [Epoch 1, Mini-batch   400] Loss: 0.178
    [Epoch 1, Mini-batch   600] Loss: 0.174
    [Epoch 2, Mini-batch   200] Loss: 0.166
    [Epoch 2, Mini-batch   400] Loss: 0.168
    [Epoch 2, Mini-batch   600] Loss: 0.166
    [Epoch 3, Mini-batch   200] Loss: 0.166
    [Epoch 3, Mini-batch   400] Loss: 0.162
    [Epoch 3, Mini-batch   600] Loss: 0.166
    [Epoch 4, Mini-batch   200] Loss: 0.161
    [Epoch 4, Mini-batch   400] Loss: 0.162
    [Epoch 4, Mini-batch   600] Loss: 0.165
    [Epoch 5, Mini-batch   200] Loss: 0.163
    [Epoch 5, Mini-batch   400] Loss: 0.161
    [Epoch 5, Mini-batch   600] Loss: 0.163
    [Epoch 6, Mini-batch   200] Loss: 0.164
    [Epoch 6, Mini-batch   400] Loss: 0.163
    [Epoch 6, Mini-batch   600] Loss: 0.162
    [Epoch 7, Mini-batch   200] Loss: 0.163
    [Epoch 7, Mini-batch   400] Loss: 0.168
    [Epoch 7, Mini-batch   600] Loss: 0.162
    [Epoch 8, Mini-batch   200] Loss: 0.164
    [Epoch 8, Mini-batch   400] Loss: 0.164
    [Epoch 8, Mini-batch   600] Loss: 0.165
    [Epoch 9, Mini-batch   200] Loss: 0.168
    [Epoch 9, Mini-batch   400] Loss: 0.168
    [Epoch 9, Mini-batch   600] Loss: 0.172
    [Epoch 10, Mini-batch   200] Loss: 0.172
    [Epoch 10, Mini-batch   400] Loss: 0.170
    [Epoch 10, Mini-batch   600] Loss: 0.168

{"model\_id":"3d8be69523e4463b828e82fa900b16f7","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%} .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% } .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }

### Run history:

  

Accuracy

‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà

Epoch

‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà

Loss

‚ñà‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ

  

### Run summary:

  

Accuracy

42.27344

Epoch

10

Loss

0.168

  

View run **sparkling-sweep-4** at: [https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/ettfrm5h](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/ettfrm5h)  
Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)

Find logs at: `./wandb/run-20240307_163323-ettfrm5h/logs`

    wandb: Agent Starting Run: sli7lpw0 with config:
    wandb:  batch_size: 64
    wandb:  epochs: 10
    wandb:  learning_rate: 0.02990947205127064

{"model\_id":"7e0e0f35a06a4c569275d28e4a780ec6","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

Tracking run with wandb version 0.16.4

Run data is saved locally in `/Users/ruangguru/Projects/ai-bootcamp/09_mlops/01_wandb/wandb/run-20240307_163623-sli7lpw0`

Syncing run **[earnest-sweep-5](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/sli7lpw0)** to [Weights & Biases](https://wandb.ai/ricky-kurniawan/cifar10-classification) ([docs](https://wandb.me/run))  
Sweep page: [https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28](https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28)

View project at [https://wandb.ai/ricky-kurniawan/cifar10-classification](https://wandb.ai/ricky-kurniawan/cifar10-classification)

View sweep at [https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28](https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28)

View run at [https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/sli7lpw0](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/sli7lpw0)

    [Epoch 1, Mini-batch   200] Loss: 0.207
    [Epoch 1, Mini-batch   400] Loss: 0.169
    [Epoch 1, Mini-batch   600] Loss: 0.157
    [Epoch 2, Mini-batch   200] Loss: 0.139
    [Epoch 2, Mini-batch   400] Loss: 0.137
    [Epoch 2, Mini-batch   600] Loss: 0.134
    [Epoch 3, Mini-batch   200] Loss: 0.125
    [Epoch 3, Mini-batch   400] Loss: 0.123
    [Epoch 3, Mini-batch   600] Loss: 0.125
    [Epoch 4, Mini-batch   200] Loss: 0.116
    [Epoch 4, Mini-batch   400] Loss: 0.118
    [Epoch 4, Mini-batch   600] Loss: 0.117
    [Epoch 5, Mini-batch   200] Loss: 0.110
    [Epoch 5, Mini-batch   400] Loss: 0.113
    [Epoch 5, Mini-batch   600] Loss: 0.113
    [Epoch 6, Mini-batch   200] Loss: 0.105
    [Epoch 6, Mini-batch   400] Loss: 0.109
    [Epoch 6, Mini-batch   600] Loss: 0.110
    [Epoch 7, Mini-batch   200] Loss: 0.101
    [Epoch 7, Mini-batch   400] Loss: 0.104
    [Epoch 7, Mini-batch   600] Loss: 0.109
    [Epoch 8, Mini-batch   200] Loss: 0.099
    [Epoch 8, Mini-batch   400] Loss: 0.105
    [Epoch 8, Mini-batch   600] Loss: 0.101
    [Epoch 9, Mini-batch   200] Loss: 0.096
    [Epoch 9, Mini-batch   400] Loss: 0.102
    [Epoch 9, Mini-batch   600] Loss: 0.103
    [Epoch 10, Mini-batch   200] Loss: 0.093
    [Epoch 10, Mini-batch   400] Loss: 0.101
    [Epoch 10, Mini-batch   600] Loss: 0.104

{"model\_id":"fd86b2e81f404f598c3fa1bb63809682","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%} .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% } .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }

### Run history:

  

Accuracy

‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá

Epoch

‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà

Loss

‚ñà‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ

  

### Run summary:

  

Accuracy

64.03125

Epoch

10

Loss

0.10403

  

View run **earnest-sweep-5** at: [https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/sli7lpw0](https://wandb.ai/ricky-kurniawan/cifar10-classification/runs/sli7lpw0)  
Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)

Find logs at: `./wandb/run-20240307_163623-sli7lpw0/logs`

    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: rtnk6ct2 with config:
    wandb:  batch_size: 64
    wandb:  epochs: 15
    wandb:  learning_rate: 0.04013296976488706
    wandb: Agent Starting Run: bqo62g9l with config:
    wandb:  batch_size: 64
    wandb:  epochs: 15
    wandb:  learning_rate: 0.019177756444346203
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 4xg5m9q5 with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.08786538663827181
    wandb: Agent Starting Run: hccvb2aj with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.09598692806072671
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 9l2kwop6 with config:
    wandb:  batch_size: 64
    wandb:  epochs: 15
    wandb:  learning_rate: 0.02399552057250517
    wandb: Agent Starting Run: kxjxgcoz with config:
    wandb:  batch_size: 64
    wandb:  epochs: 10
    wandb:  learning_rate: 0.001797124637780443
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 0hg7ms36 with config:
    wandb:  batch_size: 64
    wandb:  epochs: 5
    wandb:  learning_rate: 0.039904043596502
    wandb: Agent Starting Run: edi3pf6o with config:
    wandb:  batch_size: 64
    wandb:  epochs: 5
    wandb:  learning_rate: 0.05230665673262373
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: b0rhoirl with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.07634240948572275
    wandb: Agent Starting Run: b3kwtw9q with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.01182721332560279
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 8384njvv with config:
    wandb:  batch_size: 32
    wandb:  epochs: 10
    wandb:  learning_rate: 0.03237160620647384
    wandb: Agent Starting Run: 1s1lmbyu with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.006803383495698264
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: hmillp2c with config:
    wandb:  batch_size: 16
    wandb:  epochs: 5
    wandb:  learning_rate: 0.08159207853619327
    wandb: Agent Starting Run: 3qhajxuy with config:
    wandb:  batch_size: 64
    wandb:  epochs: 5
    wandb:  learning_rate: 0.034446125391791926
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 47338itd with config:
    wandb:  batch_size: 16
    wandb:  epochs: 5
    wandb:  learning_rate: 0.027886490017400307
    wandb: Agent Starting Run: n1vn81r5 with config:
    wandb:  batch_size: 16
    wandb:  epochs: 15
    wandb:  learning_rate: 0.06457802029413433
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: cgale92f with config:
    wandb:  batch_size: 32
    wandb:  epochs: 10
    wandb:  learning_rate: 0.09884957347400788
    wandb: Agent Starting Run: b4dzya1c with config:
    wandb:  batch_size: 16
    wandb:  epochs: 5
    wandb:  learning_rate: 0.023560550660514372
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: xc1w68kc with config:
    wandb:  batch_size: 16
    wandb:  epochs: 5
    wandb:  learning_rate: 0.0677273337735739
    wandb: Agent Starting Run: 3qc17zvp with config:
    wandb:  batch_size: 32
    wandb:  epochs: 10
    wandb:  learning_rate: 0.020872029295613256
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: bycum4ax with config:
    wandb:  batch_size: 32
    wandb:  epochs: 5
    wandb:  learning_rate: 0.0009502174683948892
    wandb: Agent Starting Run: ea13uxm4 with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.03875932439917796
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: cv9ymo3e with config:
    wandb:  batch_size: 64
    wandb:  epochs: 5
    wandb:  learning_rate: 0.03643050258260523
    wandb: Agent Starting Run: rdudww8g with config:
    wandb:  batch_size: 16
    wandb:  epochs: 15
    wandb:  learning_rate: 0.07521550538459543
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: j0t2l6xn with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.0926547142606578
    wandb: Agent Starting Run: hjs4wpup with config:
    wandb:  batch_size: 16
    wandb:  epochs: 5
    wandb:  learning_rate: 0.03103738494870234
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: a1zb8bt0 with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.08247355964474885
    wandb: Agent Starting Run: 6w9frll1 with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.04725600655064744
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: zncziwff with config:
    wandb:  batch_size: 32
    wandb:  epochs: 5
    wandb:  learning_rate: 0.043450210349017096
    wandb: Agent Starting Run: we8x72vm with config:
    wandb:  batch_size: 64
    wandb:  epochs: 15
    wandb:  learning_rate: 0.06386487328353212
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: waxypahb with config:
    wandb:  batch_size: 32
    wandb:  epochs: 5
    wandb:  learning_rate: 0.042724514638329464
    wandb: Agent Starting Run: joi5zjsj with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.019430254130361313
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: xx88hfkr with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.07547491698020012
    wandb: Agent Starting Run: kjhokwe7 with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.00019885257735111447
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: snjtvnuw with config:
    wandb:  batch_size: 64
    wandb:  epochs: 15
    wandb:  learning_rate: 0.01991671460892379
    wandb: Agent Starting Run: a8335nge with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.01795560439131162
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: d8qd3tqr with config:
    wandb:  batch_size: 16
    wandb:  epochs: 5
    wandb:  learning_rate: 0.04822296635022568
    wandb: Agent Starting Run: oynds7y0 with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.044998018963751635
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 9ll122fe with config:
    wandb:  batch_size: 64
    wandb:  epochs: 5
    wandb:  learning_rate: 0.07340241778955124
    wandb: Agent Starting Run: c0vlax52 with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.002283546818615913
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: bt6cj0un with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.022668240334927293
    wandb: Agent Starting Run: mrdz8w9o with config:
    wandb:  batch_size: 64
    wandb:  epochs: 5
    wandb:  learning_rate: 0.05708704882360202
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: umbfbt8n with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.08304445886730834
    wandb: Agent Starting Run: 3toy7vas with config:
    wandb:  batch_size: 32
    wandb:  epochs: 10
    wandb:  learning_rate: 0.08614471640332713
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: c6esvwxi with config:
    wandb:  batch_size: 32
    wandb:  epochs: 5
    wandb:  learning_rate: 0.00975734470018658
    wandb: Agent Starting Run: h3jc09p7 with config:
    wandb:  batch_size: 64
    wandb:  epochs: 5
    wandb:  learning_rate: 0.0006792195004773297
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: zm9y7f8f with config:
    wandb:  batch_size: 16
    wandb:  epochs: 15
    wandb:  learning_rate: 0.035608281690541546
    wandb: Agent Starting Run: flc4dlyi with config:
    wandb:  batch_size: 16
    wandb:  epochs: 15
    wandb:  learning_rate: 0.0747376446577179
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: d6zjnt74 with config:
    wandb:  batch_size: 64
    wandb:  epochs: 5
    wandb:  learning_rate: 0.0147811451253074
    wandb: Agent Starting Run: curacom8 with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.03078741176536328
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: dz7jp1uz with config:
    wandb:  batch_size: 16
    wandb:  epochs: 15
    wandb:  learning_rate: 0.03647067772864392
    wandb: Agent Starting Run: 88b26sl8 with config:
    wandb:  batch_size: 64
    wandb:  epochs: 10
    wandb:  learning_rate: 0.03164228010599915
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 1vx8j98a with config:
    wandb:  batch_size: 64
    wandb:  epochs: 5
    wandb:  learning_rate: 0.060367573514628875
    wandb: Agent Starting Run: 0uh1isax with config:
    wandb:  batch_size: 64
    wandb:  epochs: 5
    wandb:  learning_rate: 0.09465268340140094
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: houw5pqz with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.04899465514091969
    wandb: Agent Starting Run: w2o09wx9 with config:
    wandb:  batch_size: 32
    wandb:  epochs: 10
    wandb:  learning_rate: 0.07761409080397712
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 6kmdrt5z with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.03191007075831756
    wandb: Agent Starting Run: eyedp7sg with config:
    wandb:  batch_size: 16
    wandb:  epochs: 15
    wandb:  learning_rate: 0.0389982675078401
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: knj1pboz with config:
    wandb:  batch_size: 64
    wandb:  epochs: 15
    wandb:  learning_rate: 0.05755347056457952
    wandb: Agent Starting Run: uxyumcsf with config:
    wandb:  batch_size: 16
    wandb:  epochs: 5
    wandb:  learning_rate: 0.08216783864067907
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: f31dej07 with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.08075061578012568
    wandb: Agent Starting Run: fs20edf9 with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.07661993280501622
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 78j6qti3 with config:
    wandb:  batch_size: 64
    wandb:  epochs: 10
    wandb:  learning_rate: 0.07472375363565593
    wandb: Agent Starting Run: 0rbbjcnf with config:
    wandb:  batch_size: 16
    wandb:  epochs: 5
    wandb:  learning_rate: 0.062440351458098856
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 0p5dph5h with config:
    wandb:  batch_size: 16
    wandb:  epochs: 5
    wandb:  learning_rate: 0.095907049984872
    wandb: Agent Starting Run: ftrbpjom with config:
    wandb:  batch_size: 32
    wandb:  epochs: 5
    wandb:  learning_rate: 0.03250778308923327
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: xtyertyc with config:
    wandb:  batch_size: 32
    wandb:  epochs: 10
    wandb:  learning_rate: 0.08278359995673532
    wandb: Agent Starting Run: gujy3vad with config:
    wandb:  batch_size: 16
    wandb:  epochs: 5
    wandb:  learning_rate: 0.07482464086791861
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 7xsa7jdq with config:
    wandb:  batch_size: 16
    wandb:  epochs: 15
    wandb:  learning_rate: 0.07774704322127651
    wandb: Agent Starting Run: zoodx6p9 with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.008214304338534106
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: wlhlk1y9 with config:
    wandb:  batch_size: 64
    wandb:  epochs: 10
    wandb:  learning_rate: 0.06960814841631531
    wandb: Agent Starting Run: 5hv0lxmk with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.060997982236647166
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 5ejtdrgs with config:
    wandb:  batch_size: 32
    wandb:  epochs: 10
    wandb:  learning_rate: 0.029460761958973136
    wandb: Agent Starting Run: udyjpx2s with config:
    wandb:  batch_size: 32
    wandb:  epochs: 10
    wandb:  learning_rate: 0.03138232430215001
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: qmhjuyl3 with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.026927982306497504
    wandb: Agent Starting Run: kqe4dcbu with config:
    wandb:  batch_size: 64
    wandb:  epochs: 15
    wandb:  learning_rate: 0.0905052326377824
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 48petedk with config:
    wandb:  batch_size: 64
    wandb:  epochs: 15
    wandb:  learning_rate: 0.08917413284685244
    wandb: Agent Starting Run: 9cve8j4b with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.09537012996979832
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: cbm95fdg with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.07043152434511156
    wandb: Agent Starting Run: o7m7zvkl with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.04866494456570549
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: gjpy9r0r with config:
    wandb:  batch_size: 32
    wandb:  epochs: 5
    wandb:  learning_rate: 0.07551029408789373
    wandb: Agent Starting Run: 91fp8jt4 with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.09478472201588872
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 2pu84sxr with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.017711354426403886
    wandb: Agent Starting Run: e1sgeidh with config:
    wandb:  batch_size: 16
    wandb:  epochs: 5
    wandb:  learning_rate: 0.09046440046876088
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: k25dk7xy with config:
    wandb:  batch_size: 16
    wandb:  epochs: 5
    wandb:  learning_rate: 0.091976710481526
    wandb: Agent Starting Run: c1klbar5 with config:
    wandb:  batch_size: 64
    wandb:  epochs: 15
    wandb:  learning_rate: 0.03743326691366536
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: qhqtolza with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.08201836927894077
    wandb: Agent Starting Run: 10tr16u4 with config:
    wandb:  batch_size: 16
    wandb:  epochs: 5
    wandb:  learning_rate: 0.08041062549078154
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: inlf4ymj with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.087283456718445
    wandb: Agent Starting Run: 819rpaa4 with config:
    wandb:  batch_size: 32
    wandb:  epochs: 5
    wandb:  learning_rate: 0.038763119130549355
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: odxiv32v with config:
    wandb:  batch_size: 64
    wandb:  epochs: 15
    wandb:  learning_rate: 0.01556679894974132
    wandb: Agent Starting Run: bh9pdy8l with config:
    wandb:  batch_size: 16
    wandb:  epochs: 5
    wandb:  learning_rate: 0.04817863423238899
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: y38z5nr6 with config:
    wandb:  batch_size: 32
    wandb:  epochs: 5
    wandb:  learning_rate: 0.016870862546940594
    wandb: Agent Starting Run: y9kzkujv with config:
    wandb:  batch_size: 64
    wandb:  epochs: 15
    wandb:  learning_rate: 0.06284544026485823
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: kmwm24tu with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.07492284565203791
    wandb: Agent Starting Run: i6dl9cis with config:
    wandb:  batch_size: 16
    wandb:  epochs: 15
    wandb:  learning_rate: 0.0427534134153622
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: txbvsz69 with config:
    wandb:  batch_size: 32
    wandb:  epochs: 5
    wandb:  learning_rate: 0.05326299600938453
    wandb: Agent Starting Run: ccw4uhmi with config:
    wandb:  batch_size: 32
    wandb:  epochs: 10
    wandb:  learning_rate: 0.0480183490301355
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: ivkyuvs8 with config:
    wandb:  batch_size: 64
    wandb:  epochs: 10
    wandb:  learning_rate: 0.06865922913712455
    wandb: Agent Starting Run: ci2r2guu with config:
    wandb:  batch_size: 16
    wandb:  epochs: 15
    wandb:  learning_rate: 0.07737784852558242
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 2pnrxxz0 with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.05050030078259987
    wandb: Agent Starting Run: rq54hcia with config:
    wandb:  batch_size: 16
    wandb:  epochs: 15
    wandb:  learning_rate: 0.0910912726664158
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 00clda8n with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.07824430646201114
    wandb: Agent Starting Run: uri7slp8 with config:
    wandb:  batch_size: 16
    wandb:  epochs: 10
    wandb:  learning_rate: 0.06346494197531323
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 3u5pu7dy with config:
    wandb:  batch_size: 32
    wandb:  epochs: 15
    wandb:  learning_rate: 0.09495262177733783
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: ajuxu8d8 with config:
    wandb:  batch_size: 16
    wandb:  epochs: 15
    wandb:  learning_rate: 0.02070714388688412
    wandb: Agent Starting Run: xlz9f0ad with config:
    wandb:  batch_size: 16
    wandb:  epochs: 15
    wandb:  learning_rate: 0.029069817807907232
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: yhcoekh6 with config:
    wandb:  batch_size: 16
    wandb:  epochs: 5
    wandb:  learning_rate: 0.08499498160510198
    wandb: Agent Starting Run: simg5ivw with config:
    wandb:  batch_size: 64
    wandb:  epochs: 5
    wandb:  learning_rate: 0.034975887586655756
    wandb: Sweep Agent: Waiting for job.
    wandb: Job received.
    wandb: Agent Starting Run: 90gbjyps with config:
    wandb:  batch_size: 32
    wandb:  epochs: 5
    wandb:  learning_rate: 0.08929397988150486
    wandb: Sweep Agent: Waiting for job.
    wandb: Sweep Agent: Exiting.

This command will start the sweep we just defined. WandB will call the `train` function with the different combinations of hyperparameters defined in the sweep configuration.

Here‚Äôs what the Sweep Dashboard looks like:

![](https://storage.googleapis.com/rg-ai-bootcamp/mlops/wandb-sweep.png)

We can clearly see the which combination is more effective by looking at the charts, we can then dive in to fine tune our hyperparameters.

WandB‚Äôs sweeps are a powerful tool for optimizing your model‚Äôs hyperparameters. By integrating WandB with your model training code, you can automate the process of training many models with different hyperparameters, and then easily compare their performance on the WandB‚Äôs dashboard.

You can view the full project at this link: [CIFAR10 WanDB Dashboard](https://wandb.ai/ricky-kurniawan/cifar10-classification/overview?workspace=user-ricky-kurniawan)

Back to top


¬© Ruangguru Engineering 2024. All rights reserved