Stable Diffusion - Basic ‚Äì Mastering AI Bootcamp 

[Mastering AI Bootcamp](../../../index.html)

Stable Diffusion - Basic
========================

*   Toolkit
    
    *   Python
        
        *   Jupyter Notebook
            
            *   [Jupyter Notebooks](../../../01_toolkits/00_python/00_jupyter-notebooks/00_installation-and-setup.html)
                
            *   [Notebook cells](../../../01_toolkits/00_python/00_jupyter-notebooks/01_notebook-cells.html)
                
            *   [Markdown and Formatting](../../../01_toolkits/00_python/00_jupyter-notebooks/02_markdown-and-formatting.html)
                
            *   [Kernel Management](../../../01_toolkits/00_python/00_jupyter-notebooks/03_kernel-management.html)
                
            *   [Magic Commands](../../../01_toolkits/00_python/00_jupyter-notebooks/04_magic-commands.html)
                
            *   [JupyterLab](../../../01_toolkits/00_python/00_jupyter-notebooks/05_jupyterlab.html)
                
        *   [Python Basics](../../../01_toolkits/00_python/01_python-basics.html)
            
        *   [Control Structures](../../../01_toolkits/00_python/02_control-structures.html)
            
        *   [Function](../../../01_toolkits/00_python/03_function.html)
            
        *   [Data Structures](../../../01_toolkits/00_python/04_data-structures.html)
            
        *   [Modules and Packages](../../../01_toolkits/00_python/05_modules-and-packages.html)
            
        *   [Working with Files](../../../01_toolkits/00_python/06_working-with-files.html)
            
        *   [Virtual Environments](../../../01_toolkits/00_python/07_virtual-environments.html)
            
        *   OOP
            
            *   [Object oriented programming: Fitting functionality into single objects](../../../01_toolkits/00_python/08_oop/00_oop.html)
                
            *   [Inheritance](../../../01_toolkits/00_python/08_oop/01_oop_inheritance.html)
                
    *   Version Control
        
        *   [Introduction to Version Control](../../../01_toolkits/01_version-control/00_introduction-to-version-control.html)
            
        *   [Git Introduction](../../../01_toolkits/01_version-control/01_introduction-git.html)
            
        *   Git Command
            
            *   [`Init`](../../../01_toolkits/01_version-control/02_git-command-local/00_init.html)
                
            *   [Snapshotting](../../../01_toolkits/01_version-control/02_git-command-local/01_snapshotting.html)
                
            *   [`git remote add`](../../../01_toolkits/01_version-control/02_git-command-local/02_git-remote.html)
                
            *   [Basic branch and merge](../../../01_toolkits/01_version-control/02_git-command-local/03_basic-branch-and-merge.html)
                
            *   [`.gitignore`](../../../01_toolkits/01_version-control/02_git-command-local/04_gitignore.html)
                
        *   [Github](../../../01_toolkits/01_version-control/03_github.html)
            
    *   Fundamental of Statistics
        
        *   [Descriptive Statistics](../../../01_toolkits/02_foundational_statistics/00_descriptive_statistics.html)
            
        *   [Fundamentals of Probability](../../../01_toolkits/02_foundational_statistics/01_fundamentals_of_probability.html)
            
        *   [Data Analysis and Interpretation](../../../01_toolkits/02_foundational_statistics/02_data_analysis_and_interpretation.html)
            
        *   [Distributions in Statistics](../../../01_toolkits/02_foundational_statistics/03_distributions_statistics.html)
            
        *   [Bayesian Theory](../../../01_toolkits/02_foundational_statistics/04_bayesian_theory.html)
            
    *   [Pandas](../../../01_toolkits/03_pandas/00_pandas.html)
        
    *   [Visualization](../../../01_toolkits/04_visualization/00_visualization.html)
        
*   * * *
    
*   Machine Learning Fundamental
    
    *   [What is Machine Learning](../../../02_machine-learning-fundamental/00_what-is-machine-learning.html)
        
    *   [Introduction to Machine Learning](../../../02_machine-learning-fundamental/01_machine-learning-fundamental.html)
        
    *   [Linear Regression](../../../02_machine-learning-fundamental/02_linear-equation.html)
        
    *   [Supervised Learning](../../../02_machine-learning-fundamental/03_supervised-learning.html)
        
    *   [Scalar](../../../02_machine-learning-fundamental/04_matrix-and-vector.html)
        
    *   [Visualizing linear algebra as vector spaces](../../../02_machine-learning-fundamental/05_linear-algebra.html)
        
    *   [k-Nearest Neighbors (kNN)](../../../02_machine-learning-fundamental/06_knn.html)
        
    *   [Classification](../../../02_machine-learning-fundamental/07_classification.html)
        
    *   [Decision Tree](../../../02_machine-learning-fundamental/08_decision-tree.html)
        
    *   [Support Vector Machines](../../../02_machine-learning-fundamental/09_svm.html)
        
    *   [Direction of a vector](../../../02_machine-learning-fundamental/10_supplemental_svm.html)
        
    *   [Unsupervised Learning](../../../02_machine-learning-fundamental/11_unsupervised-learning.html)
        
    *   [Anomaly Detection](../../../02_machine-learning-fundamental/12_anomaly-detection.html)
        
    *   [Principal Component Analysis](../../../02_machine-learning-fundamental/13_pca.html)
        
    *   [Covariance formula and variance](../../../02_machine-learning-fundamental/14_supplemental-pca.html)
        
*   * * *
    
*   Deep Learning
    
    *   [Deep Learning](../../../03_deep-learning/00_deep-learning.html)
        
    *   [Deep Learning Model](../../../03_deep-learning/01_deep-learning-model.html)
        
    *   [Calculus](../../../03_deep-learning/02_calculus-for-deep-learning.html)
        
    *   [Gradient Descend and Backpropagation](../../../03_deep-learning/03_gradient-descent-and-backpropagation.html)
        
    *   [Pytorch Basic](../../../03_deep-learning/04_pytorch-basic.html)
        
    *   [Gradient Descent with PyTorch](../../../03_deep-learning/05_pytorch-gradient-descent.html)
        
    *   [Pytorch Dimension Modification](../../../03_deep-learning/06_pytorch-dimension.html)
        
    *   [Pytorch Application](../../../03_deep-learning/07_pytorch-application.html)
        
    *   [Pytorch MNIST](../../../03_deep-learning/08_pytorch-mnist.html)
        
*   * * *
    
*   Model Usage
    
    *   Introduction
        
        *   [Hugging Face](../../../04_model-usage/00-introduction/00_transformers_hugging_face.html)
            
    *   Pipeline
        
        *   [Pipelines](../../../04_model-usage/01-pipeline/00_pipeline.html)
            
        *   [Choosing the Right Processing Unit for Machine Learning: CPU, GPU, or TPU?](../../../04_model-usage/01-pipeline/01_cpu-gpu-tpu.html)
            
    *   Model Hub
        
        *   [Model Hub üóÉÔ∏è](../../../04_model-usage/02-model-hub/00_models.html)
            
    *   Transfer Learning
        
        *   [Transfer Learning](../../../04_model-usage/03-transfer-learning/00_transfer_learning.html)
            
        *   [Model Deployment](../../../04_model-usage/03-transfer-learning/01_model_deployment.html)
            
    *   Gradio
        
        *   [Gradio](../../../04_model-usage/04-gradio/00_gradio.html)
            
*   * * *
    
*   Database
    
    *   SQL
        
        *   [SQL Fundamentals with Python - Database](../../../05_database/00_sql/00_sql-database.html)
            
        *   [SQL Fundamentals with Python - Tables](../../../05_database/00_sql/01_sql-table.html)
            
        *   [SQL Fundamentals with Python - Joins](../../../05_database/00_sql/02_sql-joins.html)
            
    *   Elasticsearch
        
        *   [Introduction to Elasticsearch](../../../05_database/01_elasticsearch/00_introduction.html)
            
        *   [Installation and Configuration](../../../05_database/01_elasticsearch/01_installation-and-configuration.html)
            
        *   [Data Modeling](../../../05_database/01_elasticsearch/02_data-modeling.html)
            
        *   [Elasticsearch In Practice](../../../05_database/01_elasticsearch/03_elasticsearch-in-practice.html)
            
        *   [Query](../../../05_database/01_elasticsearch/04_query.html)
            
        *   [Snapshot](../../../05_database/01_elasticsearch/05_snapshot.html)
            
*   * * *
    
*   Computer Vision
    
    *   CNN
        
        *   [Computer Vision](../../../06_computer-vision/00_cnn/00_cnn-intro.html)
            
        *   [Hello World in Image Classification](../../../06_computer-vision/00_cnn/01_nn-vs-cnn.html)
            
        *   [CIFAR10 comparison for regular Neural Network vs CNN](../../../06_computer-vision/00_cnn/02_nn-vs-cnn-cifar10.html)
            
        *   [Convolution Layer](../../../06_computer-vision/00_cnn/03_convolution-layer.html)
            
        *   [POOLING LAYER](../../../06_computer-vision/00_cnn/04_pooling-layer.html)
            
        *   [Fully Connected Layer](../../../06_computer-vision/00_cnn/05_fully-connected-layer.html)
            
        *   [Training](../../../06_computer-vision/00_cnn/06_training.html)
            
        *   [Pretrained CNN Model](../../../06_computer-vision/00_cnn/07_pretrained-model.html)
            
        *   [Applied CNN: Object Detection and YOLO in Action](../../../06_computer-vision/00_cnn/08_object-detection.html)
            
    *   Stable Diffusion
        
        *   Introduction
            
            *   [Intro to Stable Diffusion](../../../06_computer-vision/01_stable-diffusion/00_intro/intro.html)
                
        *   Basic
            
            *   [Stable Diffusion - Basic](../../../06_computer-vision/01_stable-diffusion/01_basic/Stable_Diffusion_Basic.html)
                
        *   Fine Tuning
            
            *   [Stable Difussion Fine-tuning with Dreambooth](../../../06_computer-vision/01_stable-diffusion/02_fine_tuning/Stable_Diffusion_Fine_tuning.html)
                
        *   ControlNet
            
            *   [ControlNet with Stable Diffusion](../../../06_computer-vision/01_stable-diffusion/03_controlnet/Stable_Diffusion_ControlNet.html)
                
*   * * *
    
*   NLP
    
    *   [Intuition](../../../07_nlp/00_intuition.html)
        
    *   [Preprocessing](../../../07_nlp/01_preprocess.html)
        
    *   [Feature extraction](../../../07_nlp/02_feature_extraction.html)
        
    *   [Second architecture: Using Word Embedding for sentiment classification](../../../07_nlp/03_word_embedding_intuition.html)
        
    *   [ASCII](../../../07_nlp/04_word_embedding.html)
        
    *   [Generate Word Embedding With Word2Vec](../../../07_nlp/05_word2vec.html)
        
    *   [RNN](../../../07_nlp/06_RNN.html)
        
    *   [Seq2Seq With RNN](../../../07_nlp/07_Seq2Seq_with_RNN.html)
        
    *   [Problem With RNN](../../../07_nlp/08_attention.html)
        
    *   [Understanding Different Transformer Architectures](../../../07_nlp/09_understanding_different_architectures.html)
        
*   * * *
    
*   Langchain
    
    *   Prompt Engineering
        
        *   [NovelAI](../../../08_langchain/00_prompt-engineering/00_NovelAI.html)
            
        *   [Intro to Prompt in Generative AI](../../../08_langchain/00_prompt-engineering/01_intro-to-prompt-engineering.html)
            
    *   [API with FastAPI](../../../08_langchain/01_fast_api.html)
        
    *   [LangChain: A Python Library for Building NLP Applications](../../../08_langchain/02_intro_to_langchain.html)
        
    *   [Basic Usage LangChain](../../../08_langchain/03-basic_langchain.html)
        
    *   Chain
        
        *   [Chain](../../../08_langchain/04_chain/00_chain.html)
            
        *   [Chain Exercise](../../../08_langchain/04_chain/01_chain_exercise.html)
            
        *   [Chain Exercise](../../../08_langchain/04_chain/02_chain_exercise_answer.html)
            
    *   [Agent](../../../08_langchain/05_agent.html)
        
    *   [Memory and LangChain](../../../08_langchain/06_memory.html)
        
    *   [Memory Exercise Answer Key](../../../08_langchain/06-memory-exercise-answer-key.html)
        
    *   [Communicating with Embedded Data in Documents](../../../08_langchain/07.qna-with-data-in-the-document.html)
        
*   * * *
    
*   Machine Learning Operations
    
    *   Docker
        
        *   [Introduction to Docker](../../../09_mlops/00_docker/00_intro_setup_docker.html)
            
        *   [Building Docker Images for Python Apps](../../../09_mlops/00_docker/01_build_docker_images.html)
            
        *   [Deploying Python Apps with Docker Compose](../../../09_mlops/00_docker/02_deploy_docker_images.html)
            
    *   Wan DB
        
        *   [Introduction to WanDB](../../../09_mlops/01_wandb/00_wandb_intro_setup.html)
            
        *   [Dashboards in WanDB](../../../09_mlops/01_wandb/01_wandb_dashboard.html)
            
        *   [Cnn and Wandb Tracking](../../../09_mlops/01_wandb/cnn_and_wandb_tracking.html)
            
        *   [Fine-tuning a model on a text classification task](../../../09_mlops/01_wandb/Text_Classification_on_GLUE.html)
            

On this page
------------

- [Stable Diffusion - Basic](#stable-diffusion---basic)
  - [On this page](#on-this-page)
- [Stable Diffusion - Basic](#stable-diffusion---basic-1)
  - [Installing the libraries](#installing-the-libraries)
  - [Pipeline for image generation](#pipeline-for-image-generation)
    - [Creating the prompt](#creating-the-prompt)
    - [Generating the image](#generating-the-image)
    - [Display the image](#display-the-image)
    - [Saving the result](#saving-the-result)
  - [Let‚Äôs continue our experimentation.](#lets-continue-our-experimentation)
  - [Generating multiple images](#generating-multiple-images)
  - [Parameters](#parameters)
    - [Seed](#seed)
    - [Inference steps](#inference-steps)
    - [Guidance Scale (CFG / Strength)](#guidance-scale-cfg--strength)
    - [Image size (dimensions)](#image-size-dimensions)
    - [Negative prompt](#negative-prompt)
  - [Other models](#other-models)
    - [SD v1.5](#sd-v15)
    - [SD v2.x](#sd-v2x)
    - [Fine-tuned models with specific styles](#fine-tuned-models-with-specific-styles)
      - [Other models](#other-models-1)
  - [Changing the scheduler (sampler)](#changing-the-scheduler-sampler)

Stable Diffusion - Basic
========================

**`[ ! Attention ]`** It‚Äôs crucial to verify the license of the models, particularly if there‚Äôs an intention to use the obtained results for commercial purposes.

1.  It‚Äôs essential to utilize these models responsibly and ethically. They should not be employed to create or disseminate illegal or harmful content. This includes, but is not limited to, content that is violent, hateful, sexually explicit, or infringes on someone‚Äôs privacy or rights.
    
2.  As a user, the rights to the outputs generated using the model are retained. However, accountability for how these outputs are used also lies with the user. They should not be used in a manner that breaches the terms of the license or any applicable laws or regulations.
    
3.  The model can be used commercially or as a service, and the weights can be redistributed. However, if this is done, the same use restrictions as those in the original license must be included. A copy of the CreativeML OpenRAIL-M license must also be provided to all users.
    

(Licence of v1.4 e v1.5 https://huggingface.co/spaces/CompVis/stable-diffusion-license)

With that out of the way, let‚Äôs try out various things we can do with Stable Diffusion. Let‚Äôs get started!

> Note: - Some images when re-run will not be the same, even with the same seed. - Stable Diffusion is resource intensive in terms of need for GPU and large hard disk space, we may need to ‚Äúdisconnect and delete the runtime‚Äù and continue halfway through this notebook.

    !nvidia-smi

Installing the libraries
------------------------

*   Install the necessary libraries for stable diffusion
*   [xformers](https://github.com/facebookresearch/xformers) for memory optimization

    !pip install diffusers==0.11.1
    !pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers

Pipeline for image generation
-----------------------------

*   We can define with little effort a pipeline to use the Stable Diffusion model, through the [StableDiffusionPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py)

    import torch #PyTorch
    from diffusers import StableDiffusionPipeline

    pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4", torch_dtype=torch.float16)

    pipe = pipe.to('cuda') #We'll always use GPU, make sure your change your runtime to use GPU is you're on Collab

    pipe.enable_attention_slicing()
    pipe.enable_xformers_memory_efficient_attention()

Sometime during image generation, the image may come out all black, to avoid this we can disable safety checker.

    #avoid all black images, disabling it is easy, you can do this:
    pipe.safety_checker = lambda images, clip_input: (images, False)

### Creating the prompt

    prompt = 'orange cat'

### Generating the image

    img = pipe(prompt).images[0]

{"model\_id":"91fc6ecc98774fed96dce013557dcbe6","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

### Display the image

    img

//<!\[CDATA\[ window.\_\_mirage2 = {petok:"jgm3M6Tijz9n9uoHvxu8DiorQmDhWwo5QMAbRdxepIg-1800-0.0.1.1"}; //\]\]> 

![](Stable_Diffusion_Basic_files/figure-html/cell-11-output-1.png)

### Saving the result

    img.save('result.png')

Let‚Äôs continue our experimentation.
-----------------------------------

    prompt = 'photograph of orange cat, realistic, full hd'
    img = pipe(prompt).images[0]
    img

{"model\_id":"d06af75c571040a0949825bc6a8feccd","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

![](Stable_Diffusion_Basic_files/figure-html/cell-13-output-2.png)

    prompt = 'a photograph of orange cat'
    img = pipe(prompt).images[0]
    img

{"model\_id":"fc2963400850403592687ffc41ff86ff","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

![](Stable_Diffusion_Basic_files/figure-html/cell-14-output-2.png)

Generating multiple images
--------------------------

    from PIL import Image
    
    def grid_img(imgs, rows=1, cols=3, scale=1):
      assert len(imgs) == rows * cols
    
      w, h = imgs[0].size
      w, h = int(w * scale), int(h * scale)
    
      grid = Image.new('RGB', size = (cols * w, rows * h))
      grid_w, grid_h = grid.size
    
      for i, img in enumerate(imgs):
        img = img.resize((w, h), Image.ANTIALIAS)
        grid.paste(img, box=(i % cols * w, i // cols * h))
      return grid

    num_imgs = 3
    prompt = 'photograph of orange cat'
    imgs = pipe(prompt, num_images_per_prompt=num_imgs).images
    grid = grid_img(imgs, rows=1, cols=3, scale=0.75)
    grid

{"model\_id":"4172eb4b07bb4921aff6c9b067fd5ce7","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

![](Stable_Diffusion_Basic_files/figure-html/cell-16-output-2.png)

Parameters
----------

There are some parameters we can set

### Seed

We can set `seed` if we want to generate similar images.

    seed = 2000
    generator = torch.Generator('cuda').manual_seed(seed)
    img = pipe(prompt, generator=generator).images[0]
    img

{"model\_id":"ee8b29fdd947462f81a50567bdc33807","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

![](Stable_Diffusion_Basic_files/figure-html/cell-17-output-2.png)

    prompt = "photograph of orange cat"
    seed = 2000
    generator = torch.Generator("cuda").manual_seed(seed)
    imgs = pipe(prompt, num_images_per_prompt=num_imgs, generator=generator).images
    grid = grid_img(imgs, rows=1, cols=3, scale=0.75)
    grid

{"model\_id":"673c85001a6542a58b356f16fae1ab15","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

![](Stable_Diffusion_Basic_files/figure-html/cell-18-output-2.png)

    prompt = "van gogh painting of an orange cat"
    generator = torch.Generator("cuda").manual_seed(seed)
    imgs = pipe(prompt, num_images_per_prompt=num_imgs, generator=generator).images
    grid = grid_img(imgs, rows=1, cols=3, scale=0.75)
    grid

{"model\_id":"a3bb92b9895c40bfb30de6e00c3b8a62","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

![](Stable_Diffusion_Basic_files/figure-html/cell-19-output-2.png)

### Inference steps

Inference steps refer to the number of denoising steps to reach the final image. The default number of inference steps of 50. If you want faster results you can use a smaller number. If you want potentially higher quality results, you can use larger numbers.

Let‚Äôs try out running the pipeline with less denoising steps.

    prompt = "photograph of orange cat, realistic, full hd"
    generator = torch.Generator("cuda").manual_seed(seed)
    img = pipe(prompt, num_inference_steps=3, generator=generator).images[0]
    img

{"model\_id":"2d89849f60e649408a22f616b495ab61","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

![](Stable_Diffusion_Basic_files/figure-html/cell-20-output-2.png)

    import matplotlib.pyplot as plt
    
    plt.figure(figsize=(18,8))
    for i in range(1, 6):
      n_steps = i * 1
      #print(n_steps)
      generator = torch.Generator('cuda').manual_seed(seed)
      img = pipe(prompt, num_inference_steps=n_steps, generator=generator).images[0]
    
      plt.subplot(1, 5, i)
      plt.title('num_inference_steps: {}'.format(n_steps))
      plt.imshow(img)
      plt.axis('off')
    plt.show()

{"model\_id":"7e13ffbe74d747cd9331f71ce7a9b6eb","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

{"model\_id":"eb9135377ee349d7b0fd1d90d5c3906c","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

{"model\_id":"1c0a98f2cdb64c40bdbbeb46aab0d550","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

{"model\_id":"d5dca8fa1fe44416b51fdb14657efc2f","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

{"model\_id":"cd739080c8674e119cd4b2c9b1c609b6","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

![](Stable_Diffusion_Basic_files/figure-html/cell-21-output-6.png)

### Guidance Scale (CFG / Strength)

CFG stands for Classifier-Free Guidance, so CFG scale can be referred to as Classifier-Free Guidance scale.

So, before 2022, there was a method called classifier guidance. It‚Äôs a method that can balance between mode coverage and sample quality in diffusion models after training, similar to low-temperature sampling or truncation in other generative models. Essentially, classifier guidance is a mix between the score estimate from the diffusion model and the gradient from the image classifier. However, if we want to use it, we have to train an image classifier that‚Äôs different from the diffusion model.

Then, a question arises, **can we have guidance without a classifier?**

In 2022, Jonathan Ho and Tim Salimans from Google Brain demonstrated that we can use a pure generative model without a classifier. The title of their paper is ‚ÄúClassifier-Free Guidance‚Äù. They train both conditional and unconditional diffusion models together, then they combine the score estimates from both to achieve a trade-off between sample quality and diversity, similar to using classifier guidance.

It‚Äôs this CFG that Stable Diffusion uses to balance between the prompt and the Stable Diffusion model. If the CFG Scale is low, the image won‚Äôt follow the prompt. But if the CFG Scale is high, the result will be a random colorful image that doesn‚Äôt resemble the prompt at all.

The most suitable choice for CFG Scale is between 6.0 - 15.0. Lower values are good for photorealistic images, while higher values are suitable for a more artistic style.

    prompt = "a man sit in front of the door"
    
    generator = torch.Generator("cuda").manual_seed(seed)
    img = pipe(prompt, guidance_scale=7, generator=generator).images[0]
    img

{"model\_id":"9c33213d85b74a55a2fa2e47de3649da","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

![](Stable_Diffusion_Basic_files/figure-html/cell-22-output-2.png)

    plt.figure(figsize=(18,8))
    for i in range(1, 6):
    
      n_guidance = i + 3
      generator = torch.Generator("cuda").manual_seed(seed)
      img = pipe(prompt, guidance_scale=n_guidance, generator=generator).images[0]
    
      plt.subplot(1,5,i)
      plt.title('guidance_scale: {}'.format(n_guidance))
      plt.imshow(img)
      plt.axis('off')
    
    plt.show()

{"model\_id":"4694ebe2cb004c719dd7dda568b71cd9","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

{"model\_id":"90c0c24b0cdf4d4dbc4bd5422aa8ab2a","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

{"model\_id":"deb0b991a5ab4871afc9e5ff96c6c0bc","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

{"model\_id":"71abce1d7d094003921cfbaf3e9c6d05","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

{"model\_id":"02de5b6366fb49188afe3f35f51c2c90","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

![](Stable_Diffusion_Basic_files/figure-html/cell-23-output-6.png)

### Image size (dimensions)

The generated images are 512 x 512 pixels

Recommendations in case you want other dimensions:

*   make sure the height and width are multiples of 8
*   less than 512 will result in lower quality images
*   exceeding 512 in both directions (width and height) will repeat areas of the image (‚Äúglobal coherence‚Äù is lost)

> Landscape mode

    seed = 777
    prompt = "photograph of orange cat"
    generator = torch.Generator("cuda").manual_seed(seed)
    h, w = 512, 512
    img = pipe(prompt, height=h, width=w, generator=generator).images[0]
    img

{"model\_id":"c93822c025be4037bcd0b5031d82990c","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

![](Stable_Diffusion_Basic_files/figure-html/cell-24-output-2.png)

> Portrait mode

    generator = torch.Generator("cuda").manual_seed(seed)
    h, w = 768, 512
    img = pipe(prompt, height=h, width=w, generator=generator).images[0]
    img

{"model\_id":"9338c83dcd9b4682ab8007012560fa92","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

![](Stable_Diffusion_Basic_files/figure-html/cell-25-output-2.png)

### Negative prompt

We can use negative prompt to tell Stable Diffusion things we don‚Äôt want in our image.

    num_images = 3
    
    prompt = 'photograph of old car'
    neg_prompt = 'black white'
    
    imgs = pipe(prompt, negative_prompt = neg_prompt, num_images_per_prompt=num_images).images
    
    grid = grid_img(imgs, rows = 1, cols = 3, scale=0.75)
    grid

{"model\_id":"db056d594709404d8b28e8006ba41c3a","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

![](Stable_Diffusion_Basic_files/figure-html/cell-26-output-2.png)

Other models
------------

### SD v1.5

    sd15 = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
    sd15 = sd15.to('cuda')
    sd15.enable_attention_slicing()
    sd15.enable_xformers_memory_efficient_attention()

    num_imgs = 3
    
    prompt = "photograph of an old car"
    neg_prompt = 'black white'
    
    imgs = sd15(prompt, negative_prompt=neg_prompt, num_images_per_prompt=num_imgs).images
    
    grid = grid_img(imgs, rows=1, cols=3, scale=0.75)
    grid

{"model\_id":"f0bfec7ff47544a7951d1343ebcd5ece","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

![](Stable_Diffusion_Basic_files/figure-html/cell-28-output-2.png)

    prompt = "photo of a futuristic city on another planet, realistic, full hd"
    neg_prompt = 'buildings'
    
    imgs = sd15(prompt, negative_prompt = neg_prompt, num_images_per_prompt=num_imgs).images
    
    grid = grid_img(imgs, rows=1, cols=3, scale=0.75)
    grid

### SD v2.x

    sd2 = StableDiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
    sd2 = sd2.to("cuda")
    sd2.enable_attention_slicing()
    sd2.enable_xformers_memory_efficient_attention()

    prompt = "photograph of an old car"
    neg_prompt = 'black white'
    
    
    imgs = sd2(prompt, negative_prompt=neg_prompt, num_images_per_prompt=num_imgs).images
    
    grid = grid_img(imgs, rows=1, cols=3, scale=0.75)
    grid

{"model\_id":"f60bae37424649049e7ba2546c370b33","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

![](Stable_Diffusion_Basic_files/figure-html/cell-31-output-2.png)

### Fine-tuned models with specific styles

> Mo-di-diffusion (Modern Disney style)

https://huggingface.co/nitrosocke/mo-di-diffusion

    modi = StableDiffusionPipeline.from_pretrained("nitrosocke/mo-di-diffusion", torch_dtype=torch.float16)
    modi = modi.to("cuda")
    modi.enable_attention_slicing()
    modi.enable_xformers_memory_efficient_attention()

    prompt = "a photograph of an astronaut riding a horse, modern disney style"
    
    seed = 777
    generator = torch.Generator("cuda").manual_seed(seed)
    
    imgs = modi(prompt, generator=generator, num_images_per_prompt=num_imgs).images
    
    grid = grid_img(imgs, rows=1, cols=3, scale=0.75)
    grid

    prompt = "orange cat, modern disney style"
    
    generator = torch.Generator("cuda").manual_seed(seed)
    imgs = modi(prompt, generator=generator, num_images_per_prompt=3).images
    
    grid = grid_img(imgs, rows=1, cols=3, scale=0.5)
    grid

    prompt = ["albert einstein, modern disney style",
              "modern disney style old chevette driving in the desert, golden hour",
              "modern disney style delorean"]
    
    seed = 777
    print("Seed: ".format(str(seed)))
    generator = torch.Generator("cuda").manual_seed(seed)
    imgs = modi(prompt, generator=generator).images
    
    grid = grid_img(imgs, rows=1, cols=3, scale=0.75)
    grid

#### Other models

*   Classic Disney Style - https://huggingface.co/nitrosocke/classic-anim-diffusion
    
*   High resolution 3D animation - https://huggingface.co/nitrosocke/redshift-diffusion
    
*   Futuristic images - https://huggingface.co/nitrosocke/Future-Diffusion
    
*   Other animation styles:
    
*   https://huggingface.co/nitrosocke/Ghibli-Diffusion
    
*   https://huggingface.co/nitrosocke/spider-verse-diffusion
    
*   more models https://huggingface.co/models?other=stable-diffusion-diffusers
    

Changing the scheduler (sampler)
--------------------------------

We can also change the scheduler for our Stable Diffusion.

*   Available schedulers: https://huggingface.co/docs/diffusers/using-diffusers/schedulers#schedulers-summary

Default is [PNDMScheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py).

    sd15 = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
    sd15 = sd15.to("cuda")
    sd15.enable_attention_slicing()
    sd15.enable_xformers_memory_efficient_attention()

    sd15.scheduler

    seed = 777
    prompt = "a photo of a orange cat wearing sunglasses, on the beach, ocean in the background"
    generator = torch.Generator('cuda').manual_seed(seed)
    img = sd15(prompt, generator=generator).images[0]
    img

    sd15.scheduler.compatibles

    sd15.scheduler.config

    from diffusers import DDIMScheduler
    sd15.scheduler = DDIMScheduler.from_config(sd15.scheduler.config)

    generator = torch.Generator(device = 'cuda').manual_seed(seed)
    img = sd15(prompt, generator=generator).images[0]
    img

    from diffusers import LMSDiscreteScheduler
    sd15.scheduler = LMSDiscreteScheduler.from_config(sd15.scheduler.config)
    generator = torch.Generator(device = 'cuda').manual_seed(seed)
    img = sd15(prompt, num_inference_steps = 60, generator=generator).images[0]
    img

    from diffusers import EulerAncestralDiscreteScheduler
    
    sd15.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)
    
    generator = torch.Generator(device="cuda").manual_seed(seed)
    img = sd15(prompt, generator=generator, num_inference_steps=50).images[0]
    img

    from diffusers import EulerDiscreteScheduler
    
    sd15.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)
    
    generator = torch.Generator(device="cuda").manual_seed(seed)
    img = sd15(prompt, generator=generator, num_inference_steps=50).images[0]
    img

Back to top


¬© Ruangguru Engineering 2024. All rights reserved