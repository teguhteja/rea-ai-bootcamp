   ControlNet with Stable Diffusion ‚Äì Mastering AI Bootcamp 

[Mastering AI Bootcamp](../../../index.html)

ControlNet with Stable Diffusion
================================

*   Toolkit
    
    *   Python
        
        *   Jupyter Notebook
            
            *   [Jupyter Notebooks](../../../01_toolkits/00_python/00_jupyter-notebooks/00_installation-and-setup.html)
                
            *   [Notebook cells](../../../01_toolkits/00_python/00_jupyter-notebooks/01_notebook-cells.html)
                
            *   [Markdown and Formatting](../../../01_toolkits/00_python/00_jupyter-notebooks/02_markdown-and-formatting.html)
                
            *   [Kernel Management](../../../01_toolkits/00_python/00_jupyter-notebooks/03_kernel-management.html)
                
            *   [Magic Commands](../../../01_toolkits/00_python/00_jupyter-notebooks/04_magic-commands.html)
                
            *   [JupyterLab](../../../01_toolkits/00_python/00_jupyter-notebooks/05_jupyterlab.html)
                
        *   [Python Basics](../../../01_toolkits/00_python/01_python-basics.html)
            
        *   [Control Structures](../../../01_toolkits/00_python/02_control-structures.html)
            
        *   [Function](../../../01_toolkits/00_python/03_function.html)
            
        *   [Data Structures](../../../01_toolkits/00_python/04_data-structures.html)
            
        *   [Modules and Packages](../../../01_toolkits/00_python/05_modules-and-packages.html)
            
        *   [Working with Files](../../../01_toolkits/00_python/06_working-with-files.html)
            
        *   [Virtual Environments](../../../01_toolkits/00_python/07_virtual-environments.html)
            
        *   OOP
            
            *   [Object oriented programming: Fitting functionality into single objects](../../../01_toolkits/00_python/08_oop/00_oop.html)
                
            *   [Inheritance](../../../01_toolkits/00_python/08_oop/01_oop_inheritance.html)
                
    *   Version Control
        
        *   [Introduction to Version Control](../../../01_toolkits/01_version-control/00_introduction-to-version-control.html)
            
        *   [Git Introduction](../../../01_toolkits/01_version-control/01_introduction-git.html)
            
        *   Git Command
            
            *   [`Init`](../../../01_toolkits/01_version-control/02_git-command-local/00_init.html)
                
            *   [Snapshotting](../../../01_toolkits/01_version-control/02_git-command-local/01_snapshotting.html)
                
            *   [`git remote add`](../../../01_toolkits/01_version-control/02_git-command-local/02_git-remote.html)
                
            *   [Basic branch and merge](../../../01_toolkits/01_version-control/02_git-command-local/03_basic-branch-and-merge.html)
                
            *   [`.gitignore`](../../../01_toolkits/01_version-control/02_git-command-local/04_gitignore.html)
                
        *   [Github](../../../01_toolkits/01_version-control/03_github.html)
            
    *   Fundamental of Statistics
        
        *   [Descriptive Statistics](../../../01_toolkits/02_foundational_statistics/00_descriptive_statistics.html)
            
        *   [Fundamentals of Probability](../../../01_toolkits/02_foundational_statistics/01_fundamentals_of_probability.html)
            
        *   [Data Analysis and Interpretation](../../../01_toolkits/02_foundational_statistics/02_data_analysis_and_interpretation.html)
            
        *   [Distributions in Statistics](../../../01_toolkits/02_foundational_statistics/03_distributions_statistics.html)
            
        *   [Bayesian Theory](../../../01_toolkits/02_foundational_statistics/04_bayesian_theory.html)
            
    *   [Pandas](../../../01_toolkits/03_pandas/00_pandas.html)
        
    *   [Visualization](../../../01_toolkits/04_visualization/00_visualization.html)
        
*   * * *
    
*   Machine Learning Fundamental
    
    *   [What is Machine Learning](../../../02_machine-learning-fundamental/00_what-is-machine-learning.html)
        
    *   [Introduction to Machine Learning](../../../02_machine-learning-fundamental/01_machine-learning-fundamental.html)
        
    *   [Linear Regression](../../../02_machine-learning-fundamental/02_linear-equation.html)
        
    *   [Supervised Learning](../../../02_machine-learning-fundamental/03_supervised-learning.html)
        
    *   [Scalar](../../../02_machine-learning-fundamental/04_matrix-and-vector.html)
        
    *   [Visualizing linear algebra as vector spaces](../../../02_machine-learning-fundamental/05_linear-algebra.html)
        
    *   [k-Nearest Neighbors (kNN)](../../../02_machine-learning-fundamental/06_knn.html)
        
    *   [Classification](../../../02_machine-learning-fundamental/07_classification.html)
        
    *   [Decision Tree](../../../02_machine-learning-fundamental/08_decision-tree.html)
        
    *   [Support Vector Machines](../../../02_machine-learning-fundamental/09_svm.html)
        
    *   [Direction of a vector](../../../02_machine-learning-fundamental/10_supplemental_svm.html)
        
    *   [Unsupervised Learning](../../../02_machine-learning-fundamental/11_unsupervised-learning.html)
        
    *   [Anomaly Detection](../../../02_machine-learning-fundamental/12_anomaly-detection.html)
        
    *   [Principal Component Analysis](../../../02_machine-learning-fundamental/13_pca.html)
        
    *   [Covariance formula and variance](../../../02_machine-learning-fundamental/14_supplemental-pca.html)
        
*   * * *
    
*   Deep Learning
    
    *   [Deep Learning](../../../03_deep-learning/00_deep-learning.html)
        
    *   [Deep Learning Model](../../../03_deep-learning/01_deep-learning-model.html)
        
    *   [Calculus](../../../03_deep-learning/02_calculus-for-deep-learning.html)
        
    *   [Gradient Descend and Backpropagation](../../../03_deep-learning/03_gradient-descent-and-backpropagation.html)
        
    *   [Pytorch Basic](../../../03_deep-learning/04_pytorch-basic.html)
        
    *   [Gradient Descent with PyTorch](../../../03_deep-learning/05_pytorch-gradient-descent.html)
        
    *   [Pytorch Dimension Modification](../../../03_deep-learning/06_pytorch-dimension.html)
        
    *   [Pytorch Application](../../../03_deep-learning/07_pytorch-application.html)
        
    *   [Pytorch MNIST](../../../03_deep-learning/08_pytorch-mnist.html)
        
*   * * *
    
*   Model Usage
    
    *   Introduction
        
        *   [Hugging Face](../../../04_model-usage/00-introduction/00_transformers_hugging_face.html)
            
    *   Pipeline
        
        *   [Pipelines](../../../04_model-usage/01-pipeline/00_pipeline.html)
            
        *   [Choosing the Right Processing Unit for Machine Learning: CPU, GPU, or TPU?](../../../04_model-usage/01-pipeline/01_cpu-gpu-tpu.html)
            
    *   Model Hub
        
        *   [Model Hub üóÉÔ∏è](../../../04_model-usage/02-model-hub/00_models.html)
            
    *   Transfer Learning
        
        *   [Transfer Learning](../../../04_model-usage/03-transfer-learning/00_transfer_learning.html)
            
        *   [Model Deployment](../../../04_model-usage/03-transfer-learning/01_model_deployment.html)
            
    *   Gradio
        
        *   [Gradio](../../../04_model-usage/04-gradio/00_gradio.html)
            
*   * * *
    
*   Database
    
    *   SQL
        
        *   [SQL Fundamentals with Python - Database](../../../05_database/00_sql/00_sql-database.html)
            
        *   [SQL Fundamentals with Python - Tables](../../../05_database/00_sql/01_sql-table.html)
            
        *   [SQL Fundamentals with Python - Joins](../../../05_database/00_sql/02_sql-joins.html)
            
    *   Elasticsearch
        
        *   [Introduction to Elasticsearch](../../../05_database/01_elasticsearch/00_introduction.html)
            
        *   [Installation and Configuration](../../../05_database/01_elasticsearch/01_installation-and-configuration.html)
            
        *   [Data Modeling](../../../05_database/01_elasticsearch/02_data-modeling.html)
            
        *   [Elasticsearch In Practice](../../../05_database/01_elasticsearch/03_elasticsearch-in-practice.html)
            
        *   [Query](../../../05_database/01_elasticsearch/04_query.html)
            
        *   [Snapshot](../../../05_database/01_elasticsearch/05_snapshot.html)
            
*   * * *
    
*   Computer Vision
    
    *   CNN
        
        *   [Computer Vision](../../../06_computer-vision/00_cnn/00_cnn-intro.html)
            
        *   [Hello World in Image Classification](../../../06_computer-vision/00_cnn/01_nn-vs-cnn.html)
            
        *   [CIFAR10 comparison for regular Neural Network vs CNN](../../../06_computer-vision/00_cnn/02_nn-vs-cnn-cifar10.html)
            
        *   [Convolution Layer](../../../06_computer-vision/00_cnn/03_convolution-layer.html)
            
        *   [POOLING LAYER](../../../06_computer-vision/00_cnn/04_pooling-layer.html)
            
        *   [Fully Connected Layer](../../../06_computer-vision/00_cnn/05_fully-connected-layer.html)
            
        *   [Training](../../../06_computer-vision/00_cnn/06_training.html)
            
        *   [Pretrained CNN Model](../../../06_computer-vision/00_cnn/07_pretrained-model.html)
            
        *   [Applied CNN: Object Detection and YOLO in Action](../../../06_computer-vision/00_cnn/08_object-detection.html)
            
    *   Stable Diffusion
        
        *   Introduction
            
            *   [Intro to Stable Diffusion](../../../06_computer-vision/01_stable-diffusion/00_intro/intro.html)
                
        *   Basic
            
            *   [Stable Diffusion - Basic](../../../06_computer-vision/01_stable-diffusion/01_basic/Stable_Diffusion_Basic.html)
                
        *   Fine Tuning
            
            *   [Stable Difussion Fine-tuning with Dreambooth](../../../06_computer-vision/01_stable-diffusion/02_fine_tuning/Stable_Diffusion_Fine_tuning.html)
                
        *   ControlNet
            
            *   [ControlNet with Stable Diffusion](../../../06_computer-vision/01_stable-diffusion/03_controlnet/Stable_Diffusion_ControlNet.html)
                
*   * * *
    
*   NLP
    
    *   [Intuition](../../../07_nlp/00_intuition.html)
        
    *   [Preprocessing](../../../07_nlp/01_preprocess.html)
        
    *   [Feature extraction](../../../07_nlp/02_feature_extraction.html)
        
    *   [Second architecture: Using Word Embedding for sentiment classification](../../../07_nlp/03_word_embedding_intuition.html)
        
    *   [ASCII](../../../07_nlp/04_word_embedding.html)
        
    *   [Generate Word Embedding With Word2Vec](../../../07_nlp/05_word2vec.html)
        
    *   [RNN](../../../07_nlp/06_RNN.html)
        
    *   [Seq2Seq With RNN](../../../07_nlp/07_Seq2Seq_with_RNN.html)
        
    *   [Problem With RNN](../../../07_nlp/08_attention.html)
        
    *   [Understanding Different Transformer Architectures](../../../07_nlp/09_understanding_different_architectures.html)
        
*   * * *
    
*   Langchain
    
    *   Prompt Engineering
        
        *   [NovelAI](../../../08_langchain/00_prompt-engineering/00_NovelAI.html)
            
        *   [Intro to Prompt in Generative AI](../../../08_langchain/00_prompt-engineering/01_intro-to-prompt-engineering.html)
            
    *   [API with FastAPI](../../../08_langchain/01_fast_api.html)
        
    *   [LangChain: A Python Library for Building NLP Applications](../../../08_langchain/02_intro_to_langchain.html)
        
    *   [Basic Usage LangChain](../../../08_langchain/03-basic_langchain.html)
        
    *   Chain
        
        *   [Chain](../../../08_langchain/04_chain/00_chain.html)
            
        *   [Chain Exercise](../../../08_langchain/04_chain/01_chain_exercise.html)
            
        *   [Chain Exercise](../../../08_langchain/04_chain/02_chain_exercise_answer.html)
            
    *   [Agent](../../../08_langchain/05_agent.html)
        
    *   [Memory and LangChain](../../../08_langchain/06_memory.html)
        
    *   [Memory Exercise Answer Key](../../../08_langchain/06-memory-exercise-answer-key.html)
        
    *   [Communicating with Embedded Data in Documents](../../../08_langchain/07.qna-with-data-in-the-document.html)
        
*   * * *
    
*   Machine Learning Operations
    
    *   Docker
        
        *   [Introduction to Docker](../../../09_mlops/00_docker/00_intro_setup_docker.html)
            
        *   [Building Docker Images for Python Apps](../../../09_mlops/00_docker/01_build_docker_images.html)
            
        *   [Deploying Python Apps with Docker Compose](../../../09_mlops/00_docker/02_deploy_docker_images.html)
            
    *   Wan DB
        
        *   [Introduction to WanDB](../../../09_mlops/01_wandb/00_wandb_intro_setup.html)
            
        *   [Dashboards in WanDB](../../../09_mlops/01_wandb/01_wandb_dashboard.html)
            
        *   [Cnn and Wandb Tracking](../../../09_mlops/01_wandb/cnn_and_wandb_tracking.html)
            
        *   [Fine-tuning a model on a text classification task](../../../09_mlops/01_wandb/Text_Classification_on_GLUE.html)
            

On this page
------------

- [ControlNet with Stable Diffusion](#controlnet-with-stable-diffusion)
  - [On this page](#on-this-page)
- [ControlNet with Stable Diffusion](#controlnet-with-stable-diffusion-1)
    - [About the technique](#about-the-technique)
    - [ControlNet 1.0](#controlnet-10)
  - [Installing the libraries](#installing-the-libraries)
  - [Generating Images Using Edges](#generating-images-using-edges)
    - [ControlNet Model + Canny Edge](#controlnet-model--canny-edge)
    - [Loading the image](#loading-the-image)
    - [Detecting edges using Canny Edge](#detecting-edges-using-canny-edge)
  - [Generating Images Using Poses](#generating-images-using-poses)
    - [Loading the model to extract poses](#loading-the-model-to-extract-poses)
    - [Extract The Pose](#extract-the-pose)
    - [Loading the ControlNet model](#loading-the-controlnet-model)
    - [Trying Different Images and Prompts](#trying-different-images-and-prompts)
    - [Improve The Result](#improve-the-result)
  - [Exercise ControlNet](#exercise-controlnet)

ControlNet with Stable Diffusion
================================

This ControlNet is considered another way to guide the results in terms of composition and general of the image, as we have learned before.

As we know, it is possible to generate images from text, from images, and even by training custom model. However, Control Net introduces a new way to guide the generation of images, for example, we can control the Depth to Image, where both a text prompt and a depth image are used to condition the model. This allows you to get even more accurate results than the common image-to-image technique.

### About the technique

*   Paper [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543) published in February 2023
*   ControlNet was developed from the idea that only text is not enough to solve all problems in image generation.
*   First version: https://github.com/lllyasviel/ControlNet#below-is-controlnet-10
*   Diagram and additional explanation: https://github.com/lllyasviel/ControlNet#stable-diffusion‚Äìcontrolnet

Paper: https://arxiv.org/pdf/2302.05543.pdf

ControlNet is a method used to manage the behavior of a neural network. It does this by adjusting the input conditions of the building blocks of the neural network, which are called `network blocks`. For example, in a `restnet` pretrained CNN model, `residual network` is a `network block`.

### ControlNet 1.0

//<!\[CDATA\[ window.\_\_mirage2 = {petok:"jgm3M6Tijz9n9uoHvxu8DiorQmDhWwo5QMAbRdxepIg-1800-0.0.1.1"}; //\]\]> 

![](https://raw.githubusercontent.com/lllyasviel/ControlNet/main/github_page/he.png)

Image

The image illustrates how to apply a ControlNet to any neural network block.

*   The `x` and `y` represent deep features in neural networks. These are the complex representations that the network learns from the input data.
    
*   The `+` symbol refers to feature addition, which is a way of combining the information from different features.
    
*   The `c` represents an extra condition that is added to the neural network. This could be any additional information that you want the network to consider when making its predictions.
    

In implementing ControlNet, there are various techniques that can be used to condition the model. However, for this discussion, the focus will be on two specific methods:

1.  **Edge Detection using Canny Edge**

This technique involves identifying the boundaries of objects within an image. The Canny Edge Detection method is a popular algorithm that‚Äôs used to detect a wide range of edges in images. It‚Äôs used to help the model understand the shapes present in the input.

2.  **Pose Estimation using Open Pose**

This technique is about understanding the pose of a person in an image or video. Open Pose is a library that allows for real-time multi-person keypoint detection. It can identify where people are and how they are posed in an image or video. This information can be used to condition the model to understand and learn from the poses present in the input.

For more detailed information about implementing ControlNet and the various techniques used to condition the model, you can refer to the [ControlNet GitHub repository](https://github.com/lllyasviel/ControlNet). This resource provides comprehensive documentation, code examples, and further reading to help you understand and implement ControlNet effectively.

Installing the libraries
------------------------

    !pip install diffusers==0.14
    !pip install -q accelerate transformers xformers

*   `opencv-contrib-python` is a library for computer vision tasks, including edge detection using the Canny edge algorithm.
*   `controlnet-aux` is a library that contains auxiliary functions for the Control Net model.

    !pip install -q opencv-contrib-python
    !pip install -q controlnet_aux

    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
    import torch
    import cv2
    from PIL import Image
    import numpy as np

    #function show image as grid
    def grid_img(imgs, rows=1, cols=3, scale=1):
      assert len(imgs) == rows * cols
    
      w, h = imgs[0].size
      w, h = int(w*scale), int(h*scale)
    
      grid = Image.new('RGB', size=(cols*w, rows*h))
      grid_w, grid_h = grid.size
    
      for i, img in enumerate(imgs):
          img = img.resize((w,h), Image.ANTIALIAS)
          grid.paste(img, box=(i%cols*w, i//cols*h))
      return grid

Generating Images Using Edges
-----------------------------

### ControlNet Model + Canny Edge

This is the algorithm used to extract the edges of images. It will be easier to understand during the implementations.

*   More information about the model: https://huggingface.co/lllyasviel/sd-controlnet-canny

We are creating the variable control\_net\_canny\_model with the corresponding link to download it from the repository.

    controlnet_canny_model = 'lllyasviel/sd-controlnet-canny'
    control_net_canny = ControlNetModel.from_pretrained(controlnet_canny_model, torch_dtype=torch.float16)

    pipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5',
                                                             controlnet=control_net_canny,
                                                             torch_dtype=torch.float16)

    from diffusers import UniPCMultistepScheduler
    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)

    pipe.enable_attention_slicing()
    pipe.enable_xformers_memory_efficient_attention()

    pipe.enable_model_cpu_offload()

### Loading the image

Now we can load the image.

*   Image source: https://unsplash.com/pt-br/fotografias/OjhSUsHUIYM

    img = Image.open('/content/bird2.jpg')
    img

![](Stable_Diffusion_ControlNet_files/figure-html/cell-11-output-1.png)

### Detecting edges using Canny Edge

In this technique, we are going to use the Canny edge algorithm to extract only the borders of the image. So instead of sending the whole image to the algorithm, we are going to send only the borders.

We are going to create a function that will receive as parameter this image and will return the edges. We don‚Äôt need to worry about it because OpenCV has a pre-built function, so we just need to call it to extract the edges.

*   More about the algorithm: http://justin-liang.com/tutorials/canny/
*   More about the implemetation in OpenCV: https://docs.opencv.org/3.4/da/d22/tutorial\_py\_canny.html

    def canny_edge(img, low_threshold = 100, high_threshold = 200):
      img = np.array(img)
    
      img = cv2.Canny(img, low_threshold, high_threshold)
    
      img = img[:, :, None]
    
      img = np.concatenate([img, img, img], axis = 2)
    
      canny_img = Image.fromarray(img)
    
      return canny_img

we can visualize the edges.

    canny_img = canny_edge(img)
    canny_img

![](Stable_Diffusion_ControlNet_files/figure-html/cell-13-output-1.png)

we are able to visualize only the edges that have been extracted. Just a reminder that instead of sending the whole image to the algorithm, we are going to send only the edges. Then the algorithm will be able to generate new birds according to the edges.

We create a prompt, a seed for reproducibility, and a generator. Then we call the pipeline, sending the prompt and the edges of the image as parameters.

    prompt = "realistic photo of a blue bird with purple details, high quality, natural light"
    neg_prompt = ""
    
    seed = 777
    generator = torch.Generator(device="cuda").manual_seed(seed)
    
    imgs = pipe(
        prompt,
        canny_img,
        negative_prompt=neg_prompt,
        generator=generator,
        num_inference_steps=20,
    )

{"model\_id":"b44156e9009c473897bb9e3ef8e5a31d","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

    imgs.images[0]

![](Stable_Diffusion_ControlNet_files/figure-html/cell-15-output-1.png)

We can see a high-quality image that is related to the edges and is also in accordance with the prompt.

We can perform tests using different prompts and negative prompts.

    prompt = ["realistic photo of a blue bird with purple details, high quality, natural light",
              "realistic photo of a bird in new york during autumn, city in the background",
              "oil painting of a black bird in the desert, realistic, vivid, fantasy, surrealist, best quality, extremely detailed",
              "digital painting of a blue bird in space, stars and galaxy in the background, trending on artstation"]
    
    neg_prompt = ["blurred, lowres, bad anatomy, ugly, worst quality, low quality, monochrome, signature"] * len(prompt)
    
    seed = 777
    generator = torch.Generator(device="cuda").manual_seed(seed)
    
    imgs = pipe(
        prompt,
        canny_img,
        negative_prompt=neg_prompt,
        generator=generator,
        num_inference_steps=20,
    )

    grid_img(imgs.images, 1, len(prompt), scale=0.75)

    DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.
      img = img.resize((w,h), Image.ANTIALIAS)

![](Stable_Diffusion_ControlNet_files/figure-html/cell-17-output-2.png)

Let‚Äôs try with another image

    img = Image.open("/content/wolf.jpg")

    canny_img = canny_edge(img, 200, 255)
    
    grid_img([img, canny_img], 1, 2)

    DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.
      img = img.resize((w,h), Image.ANTIALIAS)

![](Stable_Diffusion_ControlNet_files/figure-html/cell-19-output-2.png)

    prompt = ["realistic photo of a wolf, high quality, natural light, full moon",
              "realistic photo of a wolf in the snow, best quality, extremely detailed",
              "oil painting of wolf the desert, canyons in the background, realistic, vivid, fantasy, surrealist, best quality, extremely detailed",
              "watercolor painting of a wolf in space, blue and purple tones, stars and earth in the background"]
    
    neg_prompt = ["blurred, lowres, bad anatomy, ugly, worst quality, low quality, monochrome, signature"] * len(prompt)
    
    seed = 777
    generator = torch.Generator(device="cuda").manual_seed(seed)
    
    imgs = pipe(
        prompt,
        canny_img,
        negative_prompt=neg_prompt,
        generator=generator,
        num_inference_steps=20,
    )

    grid_img(imgs.images, 1, len(prompt), scale=0.75)

    DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.
      img = img.resize((w,h), Image.ANTIALIAS)

![](Stable_Diffusion_ControlNet_files/figure-html/cell-21-output-2.png)

Generating Images Using Poses
-----------------------------

We will learn how to generate images using poses.

> If the desired images cannot be found, there are several online 3D software options available for creating posed images:
> 
> *   3D software to create posed images:
>     *   Magicposer: https://magicposer.com/
> *   Posemyart: https://posemy.art/

### Loading the model to extract poses

The first step is to download the model from `controlnet_aux`, a library we will import.

We will also import the `OpenposeDetector`. We will send an image to this detector and it will return the pose of that image.

    from controlnet_aux import OpenposeDetector
    pose_model = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')

### Extract The Pose

First, the image is loaded. Then, the pose is extracted using the `pose_model` function.

We will see the pose that has been extracted from the image.

The extracted keypoints represent specific points related to various body parts such as the head, shoulders, arms, hands, legs, feet, and so on.

    img_pose = Image.open('/content/pose01.jpg')

    pose = pose_model(img_pose)
    grid_img([img_pose, pose], rows=1, cols=2, scale=0.75)

    DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.
      img = img.resize((w,h), Image.ANTIALIAS)

![](Stable_Diffusion_ControlNet_files/figure-html/cell-24-output-2.png)

### Loading the ControlNet model

The next step is to load the ControlNet model.

    controlnet_pose_model = ControlNetModel.from_pretrained('thibaud/controlnet-sd21-openpose-diffusers', torch_dtype=torch.float16)
    sd_controlpose = StableDiffusionControlNetPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base',
                                                                       controlnet=controlnet_pose_model,
                                                                       torch_dtype=torch.float16)

    sd_controlpose.enable_model_cpu_offload()
    sd_controlpose.enable_attention_slicing()
    sd_controlpose.enable_xformers_memory_efficient_attention()

    from diffusers import DEISMultistepScheduler
    
    sd_controlpose.scheduler = DEISMultistepScheduler.from_config(sd_controlpose.scheduler.config)

    seed = 555
    generator = torch.Generator(device="cuda").manual_seed(seed)
    prompt = "professional photo of a young woman in the street, casual fashion, sharp focus, insanely detailed, photorealistic, sunset, side light"
    neg_prompt = "ugly, tiling, closed eyes, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, bad anatomy, watermark, signature, cut off, low contrast, underexposed, overexposed, bad art, beginner, amateur, distorted face"
    
    imgs = sd_controlpose(
        prompt,
        pose,
        negative_prompt=neg_prompt,
        num_images_per_prompt=4,
        generator=generator,
        num_inference_steps=20,
    )
    grid_img(imgs.images, 1, 4, 0.75)

{"model\_id":"94195d22bf8c4735b6b0e9fdde2ca87c","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

    DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.
      img = img.resize((w,h), Image.ANTIALIAS)

![](Stable_Diffusion_ControlNet_files/figure-html/cell-28-output-3.png)

### Trying Different Images and Prompts

Let‚Äôs switch things up and use a different pose image.

    img_pose = Image.open("man-pose.jpg")
    
    pose = pose_model(img_pose)
    
    grid_img([img_pose, pose], 1, 2, scale=0.5)

    DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.
      img = img.resize((w,h), Image.ANTIALIAS)

![](Stable_Diffusion_ControlNet_files/figure-html/cell-29-output-2.png)

    seed = 999
    generator = torch.Generator(device="cuda").manual_seed(seed)
    prompt = "professional photo of a young asian man in the office, formal fashion, smile, waring hat, sharp focus, insanely detailed, photorealistic, side light"
    neg_prompt = "ugly, tiling, closed eyes, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, bad anatomy, watermark, signature, cut off, low contrast, underexposed, overexposed, bad art, beginner, amateur, distorted face"
    
    imgs = sd_controlpose(
        prompt,
        pose,
        negative_prompt=neg_prompt,
        num_images_per_prompt=4,
        generator=generator,
        num_inference_steps=20,
    )
    grid_img(imgs.images, 1, 4, 0.75)

{"model\_id":"0e2bca70f299426499a659515d3457f1","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

    DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.
      img = img.resize((w,h), Image.ANTIALIAS)

![](Stable_Diffusion_ControlNet_files/figure-html/cell-30-output-3.png)

    seed = 123
    prompt = ["oil painting walter white wearing a suit and black hat and sunglasses, face portrait, in the desert, realistic, vivid",
              "oil painting walter white wearing a jedi brown coat, face portrait, wearing a hood, holding a cup of coffee, in another planet, realistic, vivid",
              "professional photo of walter white wearing a space suit, face portrait, in mars, realistic, vivid",
              "professional photo of walter white in the kitchen, face portrait, realistic, vivid"]
    
    neg_prompt = ["helmet, ugly, tiling, closed eyes, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, bad anatomy, watermark, signature, cut off, low contrast, underexposed, overexposed, bad art, beginner, amateur, distorted face"] * len(prompt)
    num_imgs = 1
    
    generator = torch.Generator(device="cuda").manual_seed(seed)
    imgs = sd_controlpose(
        prompt,
        pose,
        negative_prompt=neg_prompt,
        generator=generator,
        num_inference_steps=20,
    )
    grid_img(imgs.images, 1, len(prompt), 0.75)

{"model\_id":"b9e35f59d6c14947a0aed79199869ec1","version\_major":2,"version\_minor":0,"quarto\_mimetype":"application/vnd.jupyter.widget-view+json"}

    DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.
      img = img.resize((w,h), Image.ANTIALIAS)

![](Stable_Diffusion_ControlNet_files/figure-html/cell-31-output-3.png)

### Improve The Result

For enhanced results:

*   Experiment with various schedulers. Euler A is also suggested for use with ControlNet.
*   Modify the parameters (CFG, steps, etc.).
*   Employ effective negative prompts.
*   Tailor the prompt to closely match the initial pose.
*   Providing more context about the action is advisable. For instance, ‚Äúwalking in the street‚Äù typically yields better outcomes than simply ‚Äúin the street‚Äù.
*   Inpainting can be utilized to correct faces that haven‚Äôt been generated accurately.

Exercise ControlNet
-------------------

    # @title #### Student Identity
    student_id = "your student id" # @param {type:"string"}
    name = "your name" # @param {type:"string"}

    # Intalling Libs
    %pip install diffusers==0.14
    %pip install -q accelerate transformers xformers
    %pip install -q controlnet_aux
    %pip install rggrader

    # @title #### 00. Generating Images Using Poses
    from rggrader import submit_image
    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
    import torch
    import cv2
    from PIL import Image
    from controlnet_aux import OpenposeDetector
    from diffusers import DEISMultistepScheduler
    
    # TODO:
    # 1. Use the 'lllyasviel/ControlNet' model to extract the pose from the reference image. This model will allow us to understand the pose that is present in the image.
    # 2. Use the ControlNet models 'thibaud/controlnet-sd21-openpose-diffusers' and 'stabilityai/stable-diffusion-2-1-base' to generate the desired image. These models will take the pose extracted from the previous step and use it to generate a new image.
    # 3. The image generation will be based on the prompt that you input. Make sure your prompt is clear and describes the image you want to generate accurately.
    # 4. Once the image is generated, save it in the 'results' folder. This will ensure that you can easily locate and review the image later.
    # 5. Finally, select one of the generated images to upload. This image will be the final output of your exercise.
    
    # NOTE: Remember, the quality of the generated image will greatly depend on the accuracy of the pose extracted from the reference image and the clarity of your prompt.
    
    # Loading model and create output dir
    pose_model = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')
    
    # Put your code here:
    imgs = None
    
    # ---- End of your code ----

    # Saving the results
    !mkdir results
    for i, img in enumerate(imgs.images):
      img.save('results/result_{}.png'.format(i+1))

    # Submit Method
    assignment_id = "00_controlnet"
    question_id = "00_generating_images_using_poses"
    submit_image(student_id, question_id, 'your_image.png') # change 'your_image.png' to the name of the image you want to upload (eg. results/result_3.png)

Back to top

¬© Ruangguru Engineering 2024. All rights reserved